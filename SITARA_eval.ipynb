{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Use the following commands to update your system, install `ffmpeg`, upgrade `pip`, and install all the necessary Python dependencies.\n",
    "\n",
    "### 1. Update and Install `ffmpeg`\n",
    "\n",
    "```bash\n",
    "!apt-get -y update && apt-get -y install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-06T10:52:07.312972Z",
     "iopub.status.busy": "2025-09-06T10:52:07.312558Z",
     "iopub.status.idle": "2025-09-06T10:54:09.702263Z",
     "shell.execute_reply": "2025-09-06T10:54:09.700594Z",
     "shell.execute_reply.started": "2025-09-06T10:52:07.312914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!apt-get -y update && apt-get -y install ffmpeg\n",
    "!python -m pip install --upgrade pip\n",
    "!pip install \"opencv-python==4.11.0.86\" \"numpy==1.26.4\" \\\n",
    "             \"pycryptodome==3.20.0\" \"onnxruntime==1.18.1\" \\\n",
    "             \"timm==1.0.9\" \"rich==14.1.0\"\n",
    "# Torch and TF are preinstalled on Kaggle; you can pin if you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup Script\n",
    "\n",
    "This script sets up the working environment for a project by copying a repository, preparing directories, and validating data paths.  \n",
    "It ensures that all necessary components (repo, videos, and CSV index) are available before proceeding.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“ 1. Import Required Modules\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T10:57:58.125162Z",
     "iopub.status.busy": "2025-09-06T10:57:58.124394Z",
     "iopub.status.idle": "2025-09-06T10:57:58.270200Z",
     "shell.execute_reply": "2025-09-06T10:57:58.269165Z",
     "shell.execute_reply.started": "2025-09-06T10:57:58.125130Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "# Define project structure (replace with your actual dataset/repo names)\n",
    "DATASET_ROOT = Path(\"/path/to/dataset\")\n",
    "WORK_ROOT = Path(\"/path/to/working_dir\")\n",
    "\n",
    "REPO_NAME = \"YourRepoName\"\n",
    "REPO_SRC = DATASET_ROOT / REPO_NAME\n",
    "REPO_DST = WORK_ROOT / REPO_NAME\n",
    "\n",
    "# Copy the repo into working directory so it can be imported/edited freely\n",
    "if REPO_DST.exists():\n",
    "    shutil.rmtree(REPO_DST)\n",
    "shutil.copytree(REPO_SRC, REPO_DST)\n",
    "\n",
    "# Add repo to Python path\n",
    "sys.path.append(str(REPO_DST))\n",
    "\n",
    "# Outputs\n",
    "OUT_ROOT = WORK_ROOT / \"output\"\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Data files (adjust to your dataset layout)\n",
    "VIDEOS_DIR = DATASET_ROOT / \"videos\"\n",
    "CSV_PATH = DATASET_ROOT / \"video_index.csv\"\n",
    "\n",
    "# Sanity checks\n",
    "assert REPO_DST.exists(), \"Repo not found\"\n",
    "assert VIDEOS_DIR.exists(), \"Videos directory missing\"\n",
    "assert CSV_PATH.exists(), \"CSV file missing\"\n",
    "\n",
    "# Info logs\n",
    "print(f\"Repo:   {REPO_DST}\")\n",
    "print(f\"Videos: {len(list(VIDEOS_DIR.glob('*.mov')))}\")\n",
    "print(f\"CSV:    {CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Environment Configuration\n",
    "\n",
    "This script sets important environment variables to control **TensorFlow**, **JAX**, and **XLA** behavior.  \n",
    "Itâ€™s especially useful in development environments (e.g., Kaggle, Colab, or local CPU-only runs) where you want to **suppress logs**, **force CPU usage**, or **avoid unnecessary GPU allocation**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  1. Configure Environment Variables\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "# Suppress TensorFlow logs (INFO, WARNING, and ERROR)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# Disable GPU usage for TensorFlow and JAX\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# Force JAX to run on CPU only\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n",
    "\n",
    "# Prevent XLA from preallocating large GPU memory chunks\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "# Disable oneDNN optimization logs to reduce console noise\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T10:58:21.276881Z",
     "iopub.status.busy": "2025-09-06T10:58:21.276454Z",
     "iopub.status.idle": "2025-09-06T10:58:21.283780Z",
     "shell.execute_reply": "2025-09-06T10:58:21.282238Z",
     "shell.execute_reply.started": "2025-09-06T10:58:21.276850Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"       # hide TF INFO/WARN/ERROR\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"        # no GPU for TF/JAX\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"cpu\"            # JAX stays on CPU\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"  # don't grab big GPU chunks\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"      # avoid extra kernel logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ” AES Encryption & Video Processing Utilities\n",
    "\n",
    "This script provides a set of **utility functions** for secure data handling, video preprocessing, and frame selection.  \n",
    "Itâ€™s designed for machine learning pipelines that process video frames, encrypt sensitive metadata, and handle orientation and target frame extraction.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“¦ 1. Imports & Dependencies\n",
    "\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "from Crypto.Cipher import AES, PKCS1_OAEP\n",
    "from Crypto.Random import get_random_bytes\n",
    "from Crypto.PublicKey import RSA\n",
    "import base64, json, numpy as np, cv2, time, subprocess, torch, csv, ast, shlex, uuid, os\n",
    "from pathlib import Path\n",
    "\n",
    "import src.blur_functions as bf\n",
    "import src.utils as utils\n",
    "from src.backbones import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T10:58:21.287343Z",
     "iopub.status.busy": "2025-09-06T10:58:21.285698Z",
     "iopub.status.idle": "2025-09-06T10:58:21.319550Z",
     "shell.execute_reply": "2025-09-06T10:58:21.317451Z",
     "shell.execute_reply.started": "2025-09-06T10:58:21.287300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from Crypto.Cipher import AES\n",
    "from Crypto.Random import get_random_bytes\n",
    "from Crypto.Cipher import PKCS1_OAEP\n",
    "from Crypto.PublicKey import RSA\n",
    "import base64, json, numpy as np, cv2, time, subprocess, torch, csv, ast, shlex, uuid, os\n",
    "from pathlib import Path\n",
    "\n",
    "import src.blur_functions as bf\n",
    "import src.utils as utils\n",
    "from src.backbones import get_model\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"   # CPU; remove if you want GPU for embeddings\n",
    "\n",
    "def box_center(box):\n",
    "    x1, y1, x2, y2 = box\n",
    "    return ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "\n",
    "def pad(data: bytes):\n",
    "    pad_len = 16 - (len(data) % 16)\n",
    "    return data + bytes([pad_len]) * pad_len\n",
    "\n",
    "def encrypt_data_aes128(data: bytes, key: bytes) -> str:\n",
    "    cipher = AES.new(key, AES.MODE_ECB)\n",
    "    return base64.b64encode(cipher.encrypt(pad(data))).decode(\"utf-8\")\n",
    "\n",
    "def _parse_frame_list_cell(cell):\n",
    "    if cell is None: return []\n",
    "    if isinstance(cell, list): return [int(x) for x in cell]\n",
    "    s = str(cell).strip()\n",
    "    if not s: return []\n",
    "    try:\n",
    "        val = ast.literal_eval(s)\n",
    "        if isinstance(val, (list, tuple)): return [int(x) for x in val]\n",
    "    except Exception:\n",
    "        pass\n",
    "    out = []\n",
    "    for p in s.strip(\"[]\").split(\",\"):\n",
    "        p = p.strip()\n",
    "        if p and p.lstrip(\"-\").isdigit(): out.append(int(p))\n",
    "    return out\n",
    "\n",
    "def load_target_frames_for_video(csv_path, video_name_key):\n",
    "    target = set()\n",
    "    with open(csv_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            v = row.get(\"video_name\", \"\")\n",
    "            if Path(v).stem == Path(video_name_key).stem:\n",
    "                for idx in _parse_frame_list_cell(row.get(\"frame_numbers\", \"\")):\n",
    "                    target.add(int(idx))\n",
    "    return target\n",
    "\n",
    "def probe_rotation_deg(path) -> int:\n",
    "    try:\n",
    "        out = subprocess.check_output(\n",
    "            shlex.split(\n",
    "                f'ffprobe -v error -select_streams v:0 -show_entries stream_tags=rotate -of json \"{path}\"'\n",
    "            )\n",
    "        ).decode(\"utf-8\")\n",
    "        data = json.loads(out)\n",
    "        rotate = int(data.get(\"streams\", [{}])[0].get(\"tags\", {}).get(\"rotate\", \"0\"))\n",
    "        return rotate % 360\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def rotate_fixed(img, deg):\n",
    "    if deg == 90:  return cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "    if deg == 180: return cv2.rotate(img, cv2.ROTATE_180)\n",
    "    if deg == 270: return cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Configuration: Video Processing Pipeline\n",
    "\n",
    "This script defines a `Config` class that centralizes **all key parameters** for your video processing pipeline â€” including input/output paths, model locations, runtime options, and detection thresholds.  \n",
    "It also sets up public key loading for encryption.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ› ï¸ 1. Configuration Class\n",
    "\n",
    "```python\n",
    "class Config:\n",
    "    # â–¶ï¸ Input / Output Settings\n",
    "    INPUT_VIDEO_PATH   = str(VIDEOS_DIR / \"YourVideoName.mp4\")\n",
    "    OUTPUT_VIDEO_PATH  = str(OUT_ROOT / \"single\" / \"blurred.mp4\")\n",
    "    SAVE_OUTPUT        = False         # Save processed video\n",
    "    AUDIO              = False         # Include audio in output\n",
    "\n",
    "    # ðŸ¤– Models\n",
    "    DETECTOR_MODEL     = \"yunet\"       # Face detector model\n",
    "    EMBEDDING_MODEL    = \"edgeface_s_gamma_05\"\n",
    "    EMBEDDING_MODEL_PATH = str(REPO_NAME / \"src\" / \"models\" / \"edgeface_s_gamma_05.pt\")\n",
    "\n",
    "    # â±ï¸ Frame Processing\n",
    "    FRAME_SKIP         = 3             # Process every 3rd frame\n",
    "    TRACKING_SKIP      = 0             # Tracking interval\n",
    "    BLUR_ENABLED       = True          # Enable face blurring\n",
    "\n",
    "    # ðŸ”„ Video Orientation\n",
    "    ROTATION_DEGREES   = 0             # 0 => auto from metadata; use 90/180/270 to override\n",
    "\n",
    "    # ðŸ“ Tracking Thresholds\n",
    "    LANDMARK_STALENESS_THRESHOLD = 15\n",
    "    BOX_STALENESS_THRESHOLD      = 15\n",
    "    MSE_THRESHOLD                = 150\n",
    "\n",
    "    # ðŸ“¦ Detector Settings\n",
    "    SCRFD_MODEL_PATH   = str(REPO_NAME / \"src\" / \"models\" / \"det_500m.onnx\")\n",
    "    SCRFD_INPUT_SIZE   = (640, 640)\n",
    "\n",
    "    # ðŸŽ¨ Visualization Options\n",
    "    OVERLAY_LANDMARKS     = False\n",
    "    OVERLAY_DETECTOR_BOX  = False\n",
    "    DISPLAY_VIDEO         = False\n",
    "\n",
    "    # ðŸ“Š Frame Metadata\n",
    "    FRAMES_CSV_PATH   = str(CSV_PATH)\n",
    "    FRAME_JSON_DIR    = str(OUT_ROOT / \"single\" / \"frame_json\")\n",
    "    # FRAME_IMAGE_DIR   = str(OUT_ROOT / \"single\" / \"frame_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T10:58:21.321577Z",
     "iopub.status.busy": "2025-09-06T10:58:21.321052Z",
     "iopub.status.idle": "2025-09-06T10:58:21.371108Z",
     "shell.execute_reply": "2025-09-06T10:58:21.369637Z",
     "shell.execute_reply.started": "2025-09-06T10:58:21.321541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # default single-video (overridden when batching)\n",
    "    INPUT_VIDEO_PATH   = str(VIDEOS_DIR / \"YourVideoName.mp4\")\n",
    "    OUTPUT_VIDEO_PATH  = str(OUT_ROOT / \"single\" / \"blurred.mp4\")\n",
    "    SAVE_OUTPUT        = False\n",
    "    AUDIO              = False\n",
    "\n",
    "    DETECTOR_MODEL     = \"yunet\"\n",
    "    EMBEDDING_MODEL    = \"edgeface_s_gamma_05\"\n",
    "    EMBEDDING_MODEL_PATH = str(REPO_NAME / \"src\" / \"models\" / \"edgeface_s_gamma_05.pt\")\n",
    "\n",
    "    FRAME_SKIP         = 3\n",
    "    TRACKING_SKIP      = 0\n",
    "    BLUR_ENABLED       = True\n",
    "\n",
    "    # 0 => use ffprobe metadata; use 90/180/270 to force\n",
    "    ROTATION_DEGREES   = 0\n",
    "\n",
    "    LANDMARK_STALENESS_THRESHOLD = 15\n",
    "    BOX_STALENESS_THRESHOLD      = 15\n",
    "    MSE_THRESHOLD                = 150\n",
    "\n",
    "    SCRFD_MODEL_PATH   = str(REPO_NAME / \"src\" / \"models\" / \"det_500m.onnx\")\n",
    "    SCRFD_INPUT_SIZE   = (640, 640)\n",
    "\n",
    "    OVERLAY_LANDMARKS = False\n",
    "    OVERLAY_DETECTOR_BOX = False\n",
    "    DISPLAY_VIDEO     = False\n",
    "\n",
    "    FRAMES_CSV_PATH   = str(CSV_PATH)\n",
    "    FRAME_JSON_DIR    = str(OUT_ROOT / \"single\" / \"frame_json\")\n",
    "    # FRAME_IMAGE_DIR   = str(OUT_ROOT / \"single\" / \"frame_images\")\n",
    "\n",
    "PUBLIC_KEY_PATH = REPO_NAME / \"encryption\" / \"public_key.pem\"\n",
    "assert PUBLIC_KEY_PATH.exists(), \"public_key.pem not found\"\n",
    "public_key = RSA.import_key(open(PUBLIC_KEY_PATH, \"rb\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Pipeline Initialization & Model Loading\n",
    "\n",
    "This code is the **initial setup phase** of the face-processing pipeline.  \n",
    "It prepares the environment by creating necessary directories, initializing detection and landmark models, and loading a face embedding network.  \n",
    "After this step, the pipeline is ready to process video frames for detection, landmark extraction, blurring, or embedding tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ—‚ï¸ 1. Create Required Output Directories\n",
    "\n",
    "```python\n",
    "config = Config()\n",
    "\n",
    "# make sure dirs exist for the current config\n",
    "Path(config.FRAME_JSON_DIR).mkdir(parents=True, exist_ok=True)\n",
    "# Path(config.FRAME_IMAGE_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T10:58:21.374258Z",
     "iopub.status.busy": "2025-09-06T10:58:21.373737Z",
     "iopub.status.idle": "2025-09-06T10:58:21.710728Z",
     "shell.execute_reply": "2025-09-06T10:58:21.709311Z",
     "shell.execute_reply.started": "2025-09-06T10:58:21.374229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# make sure dirs exist for the current config\n",
    "Path(config.FRAME_JSON_DIR).mkdir(parents=True, exist_ok=True)\n",
    "# Path(config.FRAME_IMAGE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "detector = utils.initialize_detector(config)\n",
    "interpretor, input_details, output_details = utils.initialize_landmark_detector()\n",
    "\n",
    "if config.EMBEDDING_MODEL == \"edgeface_s_gamma_05\":\n",
    "    model_name = \"edgeface_s_gamma_05\"\n",
    "    model = get_model(model_name)\n",
    "    try:\n",
    "        state = torch.load(config.EMBEDDING_MODEL_PATH, map_location='cpu', weights_only=True)\n",
    "        model.load_state_dict(state)\n",
    "    except TypeError:\n",
    "        model.load_state_dict(torch.load(config.EMBEDDING_MODEL_PATH, map_location='cpu'))\n",
    "    model.eval()\n",
    "else:\n",
    "    model = None  # extend if you support other backbones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¥ `process_video_one(cfg: Config)`\n",
    "\n",
    "The `process_video_one()` function is the **main driver** of the video anonymization and face-processing pipeline.  \n",
    "It reads a video frame-by-frame, detects and tracks faces, extracts landmarks, blurs sensitive regions, encrypts metadata, and optionally generates embeddings â€” all while logging performance statistics.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Overview of What It Does\n",
    "\n",
    "| Stage | Purpose |\n",
    "|-------|---------|\n",
    "| ðŸ§© Initialization | Prepare variables, trackers, and video I/O |\n",
    "| ðŸŽžï¸ Frame Processing Loop | Read and process each video frame |\n",
    "| ðŸ” Face Detection & Tracking | Detect faces or track them across frames |\n",
    "| ðŸ“ Landmark Extraction | Get facial landmarks for each face |\n",
    "| ðŸ«£ Blurring & Metadata | Blur detected faces and save metadata |\n",
    "| ðŸ” Encryption & Storage | Encrypt face data and embeddings |\n",
    "| ðŸ“Š Statistics & Logging | Compute performance metrics and save logs |\n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ Initialization & Setup\n",
    "\n",
    "```python\n",
    "start_process_time = time.perf_counter()\n",
    "face_metadata = {}\n",
    "face_tracks = {}\n",
    "face_id_to_aes_key = {}\n",
    "MAX_CENTER_DIST = 60\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2,\n",
    "                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def process_video_one(cfg: Config):\n",
    "    start_process_time = time.perf_counter()\n",
    "    total_inference_times = []\n",
    "    total_blur_times = []; total_detector_times=[]; total_landmarker_times=[]\n",
    "    total_tracking_times=[]; total_mse_times=[]\n",
    "    prev_detected_face_scores = [] \n",
    "    store_scores = []\n",
    "\n",
    "    # per-run trackers\n",
    "    face_metadata = {}\n",
    "    face_tracks = {}\n",
    "    face_id_to_aes_key = {}\n",
    "    MAX_CENTER_DIST = 60\n",
    "    lk_params = dict(winSize=(15, 15), maxLevel=2,\n",
    "                     criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "    current_video_name = Path(cfg.INPUT_VIDEO_PATH).stem\n",
    "    TARGET_FRAMES = load_target_frames_for_video(cfg.FRAMES_CSV_PATH, current_video_name)\n",
    "\n",
    "    # rotation\n",
    "    meta_deg = probe_rotation_deg(cfg.INPUT_VIDEO_PATH)\n",
    "    effective_deg = cfg.ROTATION_DEGREES if cfg.ROTATION_DEGREES in (90,180,270) else meta_deg\n",
    "    print(f\"[{current_video_name}] rotation = {effective_deg}Â° (meta={meta_deg}Â°)\")\n",
    "\n",
    "    # open IO\n",
    "    cap, out, _, _ = utils.initialize_video_io(cfg)\n",
    "    if not cap or not cap.isOpened():\n",
    "        raise RuntimeError(f\"Failed to open video: {cfg.INPUT_VIDEO_PATH}\")\n",
    "\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)); h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)); fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    print(f\"[{current_video_name}] props: {w}x{h} @{fps:.2f}\")\n",
    "\n",
    "    frame_count = 0; blur_count=0; detector_count=0; landmarker_count=0; tracking_count=0; mse_thresh_count=0\n",
    "    detected_faces_boxes = []; last_landmark_boxes=[]; prev_frame=None; prev_gray=None\n",
    "    frames_since_last_box = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        detected_face_scores = []\n",
    "        frame_start_time = time.perf_counter()\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        frame_count += 1\n",
    "        frame_idx_zero = frame_count - 1\n",
    "\n",
    "        frame = rotate_fixed(frame, effective_deg)\n",
    "        frame_blur_records = []\n",
    "\n",
    "        # MSE motion gating\n",
    "        mse_values = []\n",
    "        if prev_frame is not None and detected_faces_boxes:\n",
    "            for face_region in detected_faces_boxes:\n",
    "                st = time.perf_counter()\n",
    "                mse = utils.calculate_frame_diff(prev_frame, frame, face_region)\n",
    "                en = time.perf_counter()\n",
    "                total_mse_times.append((en-st)*1000)\n",
    "                mse_values.append(mse)\n",
    "\n",
    "        run_detector = (frame_count == 1\n",
    "                        or frame_count % (cfg.FRAME_SKIP + 1) == 0\n",
    "                        or (mse_values and max(mse_values) > cfg.MSE_THRESHOLD))\n",
    "        if (mse_values and max(mse_values) > cfg.MSE_THRESHOLD):\n",
    "            mse_thresh_count += 1\n",
    "\n",
    "        output_frame = frame\n",
    "        rgb_frame = output_frame\n",
    "        prev_frame = output_frame\n",
    "        current_height, current_width = frame.shape[:2]\n",
    "        detected_face_scores = []\n",
    "\n",
    "        if run_detector:\n",
    "            detector_count += 1\n",
    "            st = time.perf_counter()\n",
    "            faces = utils.detect_faces(rgb_frame, detector, cfg)\n",
    "            #print(\"faces\", faces)\n",
    "            en = time.perf_counter(); total_detector_times.append((en-st)*1000)\n",
    "            detected_faces_boxes = [utils.expand_box_with_margin(f[\"box\"], 0.1, output_frame.shape[1], output_frame.shape[0]) for f in faces]\n",
    "            detected_face_scores = [f[\"score\"] for f in faces]\n",
    "            store_scores = copy.deepcopy(detected_face_scores)\n",
    "            #print(\"detected faces\", detected_face_scores)\n",
    "            if detected_faces_boxes:\n",
    "                frames_since_last_box = 0\n",
    "            else:\n",
    "                frames_since_last_box += 1\n",
    "                if frames_since_last_box >= cfg.BOX_STALENESS_THRESHOLD:\n",
    "                    detected_faces_boxes = []\n",
    "                face_tracks = {}\n",
    "            prev_detected_face_scores = store_scores\n",
    "        else:\n",
    "           # if not detected_face_scores and prev_detected_face_scores:\n",
    "             #   detected_face_scores = prev_detected_face_scores\n",
    "            frames_since_last_box += 1\n",
    "            if frames_since_last_box >= cfg.BOX_STALENESS_THRESHOLD:\n",
    "                detected_faces_boxes = []\n",
    "            if prev_gray is not None and (frame_count % (cfg.TRACKING_SKIP+1) == 0):\n",
    "                st = time.perf_counter()\n",
    "                detected_faces_boxes = [utils.track_box_with_optical_flow(prev_gray, rgb_frame, box, lk_params, scale_factor=0.4)[0]\n",
    "                                        for box in detected_faces_boxes]\n",
    "                en = time.perf_counter(); tracking_count += 1; total_tracking_times.append((en-st)*1000)\n",
    "\n",
    "        new_tracks = {}\n",
    "        for i, box in enumerate(detected_faces_boxes):\n",
    "            cx, cy = box_center(box)\n",
    "            matched_id = None; min_dist = float(\"inf\")\n",
    "            for face_id, info in face_tracks.items():\n",
    "                pcx, pcy = box_center(info[\"box\"])\n",
    "                dist = np.linalg.norm(np.array([cx, cy]) - np.array([pcx, pcy]))\n",
    "                if dist < MAX_CENTER_DIST and dist < min_dist:\n",
    "                    matched_id = face_id; min_dist = dist\n",
    "\n",
    "            if matched_id:\n",
    "                new_tracks[matched_id] = {\"box\": box, \"aes_key\": face_tracks[matched_id][\"aes_key\"]}\n",
    "            else:\n",
    "                new_id = str(uuid.uuid4()); new_key = get_random_bytes(16)\n",
    "                face_id_to_aes_key[new_id] = new_key\n",
    "                new_tracks[new_id] = {\"box\": box, \"aes_key\": new_key}\n",
    "\n",
    "            x1, y1, x2, y2 = box\n",
    "            if x1 < x2 and y1 < y2:\n",
    "                x1=max(x1,0); y1=max(y1,0); x2=min(x2,current_width); y2=min(y2,current_height)\n",
    "                face_roi_rgb = rgb_frame[y1:y2, x1:x2]\n",
    "                if face_roi_rgb.size > 0:\n",
    "                    st = time.perf_counter()\n",
    "                    landmarks, score = utils.get_landmarks_interpretor_with_score(face_roi_rgb, interpretor, input_details, output_details)\n",
    "                    en = time.perf_counter(); total_landmarker_times.append((en-st)*1000); landmarker_count += 1\n",
    "\n",
    "                    landmarks_scaled = utils.scale_landmarks(landmarks, x1, y1, x2-x1, y2-y1)\n",
    "\n",
    "                    face_id = matched_id if matched_id else new_id\n",
    "                    track_info = new_tracks[face_id]; aes_key = track_info[\"aes_key\"]\n",
    "\n",
    "                    frame_h, frame_w = frame.shape[:2]\n",
    "                    hull = cv2.convexHull(landmarks_scaled)\n",
    "                    hull_pts = np.array(hull).reshape(-1,2).astype(int).tolist()\n",
    "\n",
    "                    # base bounding rect\n",
    "                    x, y, w, h = cv2.boundingRect(hull)\n",
    "\n",
    "                    # scale box 1.15x\n",
    "                    scale = 1.0\n",
    "                    cx, cy = x + w/2, y + h/2\n",
    "                    new_w, new_h = w * scale, h * scale\n",
    "                    x1 = int(max(0, cx - new_w/2))\n",
    "                    y1 = int(max(0, cy - new_h/2))\n",
    "                    x2 = int(min(frame_w, cx + new_w/2))\n",
    "                    y2 = int(min(frame_h, cy + new_h/2))\n",
    "\n",
    "                    # --- discard very small faces (< 30x30 pxÂ²) ---\n",
    "                    box_area = (x2 - x1) * (y2 - y1)\n",
    "                    if box_area < 30 * 30:\n",
    "                        continue\n",
    "\n",
    "                    # save record\n",
    "                    # choose detection score with fallbacks\n",
    "                    if i < len(store_scores):\n",
    "                        det_score_val = float(store_scores[i])\n",
    "                    elif len(detected_face_scores) > 0:\n",
    "                        det_score_val = float(store_scores[-1])\n",
    "                    elif prev_detected_face_scores:\n",
    "                        det_score_val = float(prev_detected_face_scores[-1])\n",
    "                    else:\n",
    "                        det_score_val = 0.5\n",
    "                        print(\"random\")\n",
    "                    \n",
    "                    frame_blur_records.append({\n",
    "                        \"face_id\": face_id,\n",
    "                        \"pred_bbox\": [x1, y1, x2, y2],\n",
    "                        \"hull\": hull_pts,\n",
    "                        \"det_score\": det_score_val,\n",
    "                        \"land_score\": float(score)\n",
    "                    })\n",
    "                    #print(\"score\", i, len(detected_face_scores), \"â†’\", det_score_val)\n",
    "\n",
    "                    # adjust landmarks relative to cropped box\n",
    "                    landmarks_adjusted = np.array(landmarks_scaled)\n",
    "                    landmarks_adjusted[:,0] -= x1\n",
    "                    landmarks_adjusted[:,1] -= y1\n",
    "\n",
    "                    # crop face image\n",
    "                    face_image_for_encryption = rgb_frame[y1:y2, x1:x2]\n",
    "                    _, buffer = cv2.imencode('.png', face_image_for_encryption)\n",
    "                    face_image_encoded = base64.b64encode(buffer).decode('utf-8')\n",
    "\n",
    "                    data = {\n",
    "                        \"frame\": frame_count,\n",
    "                        \"landmarks\": landmarks_adjusted.tolist(),\n",
    "                        \"face_image\": face_image_encoded,\n",
    "                        \"box\": [x1, y1, x2, y2],\n",
    "                        \"box_score\": float(detected_face_scores[i]) if i < len(detected_face_scores) else float(\"nan\"),\n",
    "                        \"land_score\": float(score),\n",
    "                        \"face_id\": face_id\n",
    "                    }\n",
    "                    face_metadata.setdefault(face_id, []).append(data)\n",
    "\n",
    "                    # blur if enabled\n",
    "                    if cfg.BLUR_ENABLED:\n",
    "                        st=time.perf_counter()\n",
    "                        output_frame = bf.apply_blur_new(rgb_frame, hull, x1, y1, x2-x1, y2-y1)\n",
    "                        en=time.perf_counter(); total_blur_times.append((en-st)*1000); blur_count+=1\n",
    "\n",
    "        face_tracks = new_tracks\n",
    "        prev_gray = output_frame\n",
    "\n",
    "        # save full-video out if requested\n",
    "        if cfg.SAVE_OUTPUT and out: out.write(output_frame)\n",
    "\n",
    "        # save per-frame artifacts only for CSV target frames\n",
    "        if frame_idx_zero in TARGET_FRAMES:\n",
    "            base = f\"{current_video_name}_frame{frame_idx_zero}\"\n",
    "            json_out_path = Path(cfg.FRAME_JSON_DIR) / f\"{base}.json\"\n",
    "            json_out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(json_out_path, \"w\", encoding=\"utf-8\") as jf:\n",
    "                json.dump(frame_blur_records, jf, indent=2)\n",
    "            #img_out_path  = Path(cfg.FRAME_IMAGE_DIR)  / f\"{base}.png\"\n",
    "            #cv2.imwrite(str(img_out_path), output_frame)\n",
    "\n",
    "        frame_end_time = time.perf_counter()\n",
    "        total_inference_times.append((frame_end_time - frame_start_time)*1000)\n",
    "\n",
    "    # cleanup\n",
    "    cap.release()\n",
    "    if out: out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # postprocess encryption + embeddings\n",
    "    encrypted_face_metadata = {}; to_be_embedded = {}\n",
    "    for face_id, entries in face_metadata.items():\n",
    "        aes_key = face_id_to_aes_key[face_id]\n",
    "        encrypted_entries = []\n",
    "        best_idx = max(range(len(entries)), key=lambda k: entries[k].get(\"land_score\", -1))\n",
    "        for entry in entries:\n",
    "            encrypted_entries.append(encrypt_data_aes128(json.dumps(entry).encode(\"utf-8\"), aes_key))\n",
    "        encrypted_face_metadata[f\"frame_{face_id}\"] = encrypted_entries\n",
    "        to_be_embedded[face_id] = (entries[best_idx].get(\"face_image\"), entries[best_idx].get(\"landmarks\"))\n",
    "\n",
    "    (OUT_ROOT/\"video_metadata_encrypted.json\").write_text(json.dumps(encrypted_face_metadata, indent=2))\n",
    "\n",
    "    if to_be_embedded and (model is not None):\n",
    "        batch_tensor, face_id_order = utils.preprocess_face_images(to_be_embedded, config.EMBEDDING_MODEL)\n",
    "        if batch_tensor is not None:\n",
    "            with torch.no_grad():\n",
    "                embeddings = model(batch_tensor)\n",
    "            face_embeddings = embeddings.numpy().tolist()\n",
    "            cipher_rsa = PKCS1_OAEP.new(public_key)\n",
    "            face_id_to_combined_key_embedding = {}\n",
    "            for i, face_id in enumerate(face_id_order):\n",
    "                aes_key = face_id_to_aes_key[face_id]\n",
    "                encrypted_aes_key_b64 = base64.b64encode(cipher_rsa.encrypt(aes_key)).decode('utf-8')\n",
    "                encrypted_embedding_b64 = encrypt_data_aes128(json.dumps(face_embeddings[i]).encode('utf-8'), aes_key)\n",
    "                face_id_to_combined_key_embedding[face_id] = {\n",
    "                    \"aes_key\": encrypted_aes_key_b64,\n",
    "                    \"embedding\": encrypted_embedding_b64\n",
    "                }\n",
    "            face_id_to_combined_key_embedding[\"video_identifier\"] = str(uuid.uuid4())\n",
    "            (OUT_ROOT/\"face_keys.json\").write_text(json.dumps(face_id_to_combined_key_embedding, indent=2))\n",
    "        else:\n",
    "            (OUT_ROOT/\"face_keys.json\").write_text(json.dumps({\"video_identifier\": str(uuid.uuid4())}, indent=2))\n",
    "    else:\n",
    "        (OUT_ROOT/\"face_keys.json\").write_text(json.dumps({\"video_identifier\": str(uuid.uuid4())}, indent=2))\n",
    "\n",
    "    # stats\n",
    "        # stats\n",
    "    if total_inference_times:\n",
    "        avg_inf = sum(total_inference_times)/len(total_inference_times)\n",
    "        avg_fps = 1000/avg_inf if avg_inf>0 else 0\n",
    "        print(f\"[{current_video_name}] frames: {frame_count}  avg_ms: {avg_inf:.2f}  fps~{avg_fps:.2f}\")\n",
    "\n",
    "    # --- log total blurring time ---\n",
    "    total_blurring_time = sum(total_blur_times)  # in ms\n",
    "    blur_log = {\n",
    "        \"video_name\": str(cfg.INPUT_VIDEO_PATH),\n",
    "        \"total_blurring_time_ms\": total_blurring_time\n",
    "    }\n",
    "    blur_log_path = OUT_ROOT / \"blur_stats.json\"\n",
    "    if blur_log_path.exists():\n",
    "        # append to existing JSON (list of logs)\n",
    "        existing = json.loads(blur_log_path.read_text())\n",
    "        if isinstance(existing, list):\n",
    "            existing.append(blur_log)\n",
    "        else:\n",
    "            existing = [existing, blur_log]\n",
    "        blur_log_path.write_text(json.dumps(existing, indent=2))\n",
    "    else:\n",
    "        blur_log_path.write_text(json.dumps([blur_log], indent=2))\n",
    "\n",
    "    print(\"Done:\", current_video_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "cv2.destroyAllWindows = lambda: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Batch Video Processing & Metadata Export\n",
    "\n",
    "This script automates **batch processing** of multiple videos of our dataset.  \n",
    "It iterates over predefined subfolders, runs the full face-detection, landmark, blurring, and encryption pipeline for each video, and exports results (JSON metadata and optionally blurred videos) into organized output directories.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Overview of What It Does\n",
    "\n",
    "| Step | Purpose |\n",
    "|------|---------|\n",
    "| ðŸ“ Folder Iteration | Go through all subfolders (categories) and collect video files |\n",
    "| ðŸŽžï¸ Per-Video Processing | Run the full `process_video_one()` pipeline on each video |\n",
    "| ðŸ“Š Frame Metadata Export | Save per-frame JSON metadata for each video |\n",
    "| ðŸ“¦ Batch Summary | Count processed videos and total metadata outputs |\n",
    "| ðŸ“ Archiving | Zip all JSON metadata and optionally blurred videos for easy download |\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ 1. Folder & File Configuration\n",
    "\n",
    "```python\n",
    "FOLDERS = [\n",
    "    \"1\", \"2\", \"3\", \"4\", \"5\",\n",
    "    \"Rest\", \"Bystander_Movement\", \"Head_Movement\",\n",
    "    \"Close\", \"Far\", \"Medium\"\n",
    "]\n",
    "\n",
    "VIDEO_PATTERNS = [\"*.mov\", \"*.mp4\"]\n",
    "SAVE_BLURRED_VIDEOS = False  # toggle MP4 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Folders to iterate\n",
    "FOLDERS = [\n",
    "    \"1\", \"2\", \"3\", \"4\", \"5\",\n",
    "    \"Rest\", \"Bystander_Movement\", \"Head_Movement\",\n",
    "    \"Close\", \"Far\", \"Medium\"\n",
    "]\n",
    "\n",
    "VIDEO_PATTERNS = [\"*.mov\", \"*.mp4\"]\n",
    "SAVE_BLURRED_VIDEOS = False  # toggle MP4 output\n",
    "\n",
    "def iter_videos(folder: Path, patterns):\n",
    "    vids = []\n",
    "    for pat in patterns:\n",
    "        vids.extend(folder.glob(pat))\n",
    "    return sorted(vids, key=lambda p: p.name.lower())\n",
    "\n",
    "grand_total_videos = 0\n",
    "grand_total_jsons = 0\n",
    "\n",
    "# One common output folder for all JSONs and (optional) blurred videos\n",
    "json_dir = OUT_ROOT / \"frame_json\"\n",
    "blur_dir = OUT_ROOT / \"blurred\"\n",
    "json_dir.mkdir(parents=True, exist_ok=True)\n",
    "if SAVE_BLURRED_VIDEOS:\n",
    "    blur_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Expecting {len(FOLDERS)} subfolders under {DATASET_ROOT}\")\n",
    "\n",
    "for folder_name in FOLDERS:\n",
    "    src_dir = DATASET_ROOT / str(folder_name)\n",
    "    if not src_dir.exists() or not src_dir.is_dir():\n",
    "        print(f\"\\n=== Folder: {folder_name} | âš  not found at {src_dir}\")\n",
    "        continue\n",
    "\n",
    "    videos = iter_videos(src_dir, VIDEO_PATTERNS)\n",
    "    num_videos = len(videos)\n",
    "    print(f\"\\n=== Folder: {folder_name} | {num_videos} video(s) in {src_dir} ===\")\n",
    "\n",
    "    processed_count = 0\n",
    "    folder_json_count = 0\n",
    "\n",
    "    for idx, vpath in enumerate(videos, start=1):\n",
    "        print(f\"\\n--- [{folder_name}] {idx}/{num_videos}: {vpath.name} ---\")\n",
    "\n",
    "        # Point config at common outputs\n",
    "        config.INPUT_VIDEO_PATH = str(vpath)\n",
    "        config.FRAME_JSON_DIR = str(json_dir)\n",
    "        config.SAVE_OUTPUT = bool(SAVE_BLURRED_VIDEOS)\n",
    "        if SAVE_BLURRED_VIDEOS:\n",
    "            config.OUTPUT_VIDEO_PATH = str(blur_dir / f\"{vpath.stem}_blurred.mp4\")\n",
    "\n",
    "        # Run pipeline\n",
    "        try:\n",
    "            process_video_one(config)\n",
    "            processed_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Pipeline error for {vpath.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Count JSONs\n",
    "        saved_jsons = sorted(json_dir.glob(f\"{vpath.stem}_frame*.json\"))\n",
    "        folder_json_count += len(saved_jsons)\n",
    "\n",
    "        # Optional frame check\n",
    "        try:\n",
    "            expected_frames = load_target_frames_for_video(config.FRAMES_CSV_PATH, vpath.stem)\n",
    "        except Exception:\n",
    "            expected_frames = None\n",
    "\n",
    "        if expected_frames is not None:\n",
    "            try:\n",
    "                expected_set = set(int(x) for x in expected_frames)\n",
    "                got_frames = {int(p.stem.split(\"_frame\", 1)[-1]) for p in saved_jsons}\n",
    "                missing = sorted(expected_set - got_frames)\n",
    "                if missing:\n",
    "                    print(f\"Missing {len(missing)} expected frames. Example: {missing[:10]}\")\n",
    "                else:\n",
    "                    print(f\"All {len(got_frames)} expected frames saved.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Frame check error for {vpath.stem}: {e}\")\n",
    "        else:\n",
    "            print(f\"Saved {len(saved_jsons)} JSON files for {vpath.stem}.\")\n",
    "\n",
    "    grand_total_videos += processed_count\n",
    "    grand_total_jsons += folder_json_count\n",
    "\n",
    "    print(f\"\\n>>> Folder complete: {folder_name}\")\n",
    "    print(f\"    Videos processed: {processed_count}/{num_videos}\")\n",
    "    print(f\"    JSONs saved:      {folder_json_count}\")\n",
    "\n",
    "# --- Final zip(s) ---\n",
    "# One big zip with all JSONs\n",
    "zip_jsons = OUT_ROOT / \"all_frame_jsons.zip\"\n",
    "if zip_jsons.exists():\n",
    "    zip_jsons.unlink()\n",
    "shutil.make_archive(str(zip_jsons.with_suffix(\"\")), 'zip', root_dir=json_dir)\n",
    "print(f\"\\nZipped all JSONs â†’ {zip_jsons}\")\n",
    "\n",
    "# (Optional) one zip for blurred videos if enabled\n",
    "if SAVE_BLURRED_VIDEOS:\n",
    "    zip_videos = OUT_ROOT / \"all_blurred_videos.zip\"\n",
    "    if zip_videos.exists():\n",
    "        zip_videos.unlink()\n",
    "    shutil.make_archive(str(zip_videos.with_suffix(\"\")), 'zip', root_dir=blur_dir)\n",
    "    print(f\"Zipped all blurred videos â†’ {zip_videos}\")\n",
    "\n",
    "print(\"\\n======== Batch complete ========\")\n",
    "print(f\"Total videos processed: {grand_total_videos}\")\n",
    "print(f\"Total JSONs saved:      {grand_total_jsons}\")\n",
    "print(\"All outputs organized under:\", OUT_ROOT.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š COCO Evaluation Pipeline for Face Detection\n",
    "\n",
    "This script converts ground-truth and prediction JSON files into **COCO-compatible format** and runs a full **evaluation** using `pycocotools`.  \n",
    "It calculates standard detection metrics such as **Average Precision (AP)** and **Average Recall (AR)** at different IoU thresholds to evaluate the performance of your pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  What This Script Does\n",
    "\n",
    "| Stage | Purpose |\n",
    "|-------|---------|\n",
    "| ðŸ—‚ï¸ Load Ground Truth | Read ground truth JSONs containing annotated bounding boxes |\n",
    "| ðŸ§ª Build COCO GT | Convert ground truth to COCO format (`coco_gt.json`) |\n",
    "| ðŸ“ˆ Build COCO Results | Convert model predictions to COCO format (`coco_results.json`) |\n",
    "| ðŸ”Ž Evaluate | Run `pycocotools` evaluation and compute AP/AR metrics |\n",
    "| ðŸ“Š Summarize | Print detailed results and store them in a results dictionary |\n",
    "\n",
    "---\n",
    "\n",
    "### âš™ï¸ 1. Configuration\n",
    "\n",
    "```python\n",
    "GT_DIR  = \"YourAnnotationsFolder\"           # Folder containing *_output.json files (ground truth)\n",
    "EXP_DIR = OUT_ROOT / \"frame_json\"           # Folder containing pipeline output JSONs (predictions)\n",
    "OUTDIR  = \"../coco_eval\"                    # Directory to save COCO-format JSONs and results\n",
    "CATEGORY_NAME = \"face\"                      # Object category name\n",
    "CATEGORY_ID = 1                             # Category ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#   (also accepts \"bbox_xyxy\" instead of \"pred_bbox\"; hulls ignored)\n",
    "all_results = {}\n",
    "\n",
    "import os, json, math, numpy as np\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "# ----------------- CONFIG: CHANGE THESE -----------------\n",
    "GT_DIR  = \"YourAnnotationsFolder\"    # folder containing *_output.json files\n",
    "EXP_DIR = OUT_ROOT / \"frame_json\"   # folder containing *.json files\n",
    "OUTDIR  = \"../coco_eval\"       # where coco_gt.json / coco_results.json will be written\n",
    "CATEGORY_NAME = \"face\"\n",
    "CATEGORY_ID = 1\n",
    "# --------------------------------------------------------\n",
    "\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "COCO_GT_PATH = os.path.join(OUTDIR, \"coco_gt.json\")\n",
    "COCO_DT_PATH = os.path.join(OUTDIR, \"coco_results.json\")\n",
    "\n",
    "# --------------- Helpers ---------------\n",
    "def list_jsons(folder: str) -> List[str]:\n",
    "    return sorted([f for f in os.listdir(folder) if f.lower().endswith(\".json\")])\n",
    "\n",
    "def read_json(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def write_json(obj, path: str):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f)\n",
    "\n",
    "def xyxy_to_xywh(b: List[float]) -> List[float]:\n",
    "    x1, y1, x2, y2 = [float(v) for v in b]\n",
    "    return [x1, y1, max(0.0, x2 - x1), max(0.0, y2 - y1)]\n",
    "\n",
    "def extract_all_gt_xyxy(js: Any) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    GT: list of dicts or single dict with 'bbox_xyxy'.\n",
    "    Returns all xyxy boxes. Empty list if none.\n",
    "    \"\"\"\n",
    "    out: List[List[float]] = []\n",
    "    if isinstance(js, dict):\n",
    "        bb = js.get(\"bbox_xyxy\")\n",
    "        if isinstance(bb, list) and len(bb) == 4:\n",
    "            out.append([float(x) for x in bb])\n",
    "    elif isinstance(js, list):\n",
    "        for item in js:\n",
    "            if isinstance(item, dict):\n",
    "                bb = item.get(\"bbox_xyxy\")\n",
    "                if isinstance(bb, list) and len(bb) == 4:\n",
    "                    out.append([float(x) for x in bb])\n",
    "    return out\n",
    "\n",
    "def parse_pred_rect_xyxy(det: Dict[str, Any]) -> Optional[List[float]]:\n",
    "    \"\"\"\n",
    "    EXP: prefer 'pred_bbox', fallback to 'bbox_xyxy', both xyxy.\n",
    "    \"\"\"\n",
    "    if not isinstance(det, dict):\n",
    "        return None\n",
    "    pb = det.get(\"pred_bbox\")\n",
    "    if isinstance(pb, list) and len(pb) == 4:\n",
    "        return [float(v) for v in pb]\n",
    "    bxyxy = det.get(\"bbox_xyxy\")\n",
    "    if isinstance(bxyxy, list) and len(bxyxy) == 4:\n",
    "        return [float(v) for v in bxyxy]\n",
    "    return None\n",
    "\n",
    "def extract_all_pred_xyxy_and_scores(js: Any) -> List[Tuple[List[float], float]]:\n",
    "    \"\"\"\n",
    "    EXP: list (or dict) of detections.\n",
    "    Score priority: det_score > score > land_score > 1.0\n",
    "    NaN/inf scores -> 0.0\n",
    "    \"\"\"\n",
    "    out: List[Tuple[List[float], float]] = []\n",
    "    dets = js if isinstance(js, list) else ([js] if isinstance(js, dict) else [])\n",
    "    for d in dets:\n",
    "        if not isinstance(d, dict):\n",
    "            continue\n",
    "        box = parse_pred_rect_xyxy(d)\n",
    "        if box is None:\n",
    "            continue\n",
    "        score = None\n",
    "        for k in (\"det_score\", \"score\", \"land_score\"):\n",
    "            v = d.get(k, None)\n",
    "            if isinstance(v, (int, float)):\n",
    "                score = float(v)\n",
    "                break\n",
    "        if score is None:\n",
    "            score = 1.0\n",
    "        # sanitize to finite\n",
    "        try:\n",
    "            if not math.isfinite(score):\n",
    "                score = 0.0\n",
    "        except Exception:\n",
    "            score = 0.0\n",
    "        out.append((box, score))\n",
    "    return out\n",
    "\n",
    "# --------------- Build COCO GT from GT_DIR ---------------\n",
    "def build_coco_gt_from_gt_folder(gt_dir: str, category_id: int = 1, category_name: str = \"face\") -> Dict:\n",
    "    \"\"\"\n",
    "    Creates COCO GT with:\n",
    "      images.file_name == exp filename (i.e., GT filename with trailing '_output' removed before '.json')\n",
    "      annotations with bbox in xywh\n",
    "      includes required 'info' and 'licenses' to satisfy pycocotools expectations\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    annotations = []\n",
    "    categories = [{\"id\": category_id, \"name\": category_name}]\n",
    "    ann_id = 1\n",
    "    img_id = 1\n",
    "\n",
    "    gt_files = list_jsons(gt_dir)\n",
    "    for fname in gt_files:\n",
    "        # Map GT \"<name>_output.json\" -> EXP \"<name>.json\"\n",
    "        if fname.endswith(\"_output.json\"):\n",
    "            exp_name = fname[:-len(\"_output.json\")] + \".json\"\n",
    "        else:\n",
    "            exp_name = fname  # fallback: identical name\n",
    "\n",
    "        img_entry = {\"id\": img_id, \"file_name\": exp_name}\n",
    "        images.append(img_entry)\n",
    "\n",
    "        js = read_json(os.path.join(gt_dir, fname))\n",
    "        gt_boxes = extract_all_gt_xyxy(js)\n",
    "        for b in gt_boxes:\n",
    "            x, y, w, h = xyxy_to_xywh(b)\n",
    "            annotations.append({\n",
    "                \"id\": ann_id,\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": [x, y, w, h],\n",
    "                \"iscrowd\": 0,\n",
    "                \"area\": float(w * h),\n",
    "            })\n",
    "            ann_id += 1\n",
    "\n",
    "        img_id += 1\n",
    "\n",
    "    coco_gt = {\n",
    "        \"info\": {\n",
    "            \"description\": \"Auto-generated GT\",\n",
    "            \"version\": \"1.0\",\n",
    "            \"year\": 2025,\n",
    "            \"contributor\": \"\",\n",
    "            \"date_created\": \"\"\n",
    "        },\n",
    "        \"licenses\": [],\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        \"categories\": categories\n",
    "    }\n",
    "    return coco_gt\n",
    "\n",
    "# --------------- Build COCO Results from EXP_DIR using GT keying ---------------\n",
    "def build_coco_results_from_exp_folder(coco_gt: Dict, exp_dir: str, category_id: int = 1) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Uses coco_gt['images'] file_name to map to EXP files.\n",
    "    Returns list of detection dicts (xywh + score).\n",
    "    \"\"\"\n",
    "    # Map exp file_name -> image_id\n",
    "    name2id = {img[\"file_name\"]: img[\"id\"] for img in coco_gt.get(\"images\", [])}\n",
    "    results: List[Dict] = []\n",
    "\n",
    "    exp_files = set(list_jsons(exp_dir))\n",
    "    for exp_name, image_id in name2id.items():\n",
    "        if exp_name not in exp_files:\n",
    "            # missing prediction file: skip quietly\n",
    "            continue\n",
    "        js = read_json(os.path.join(exp_dir, exp_name))\n",
    "        preds = extract_all_pred_xyxy_and_scores(js)\n",
    "        for xyxy, score in preds:\n",
    "            x, y, w, h = xyxy_to_xywh(xyxy)\n",
    "            results.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": [x, y, w, h],\n",
    "                \"score\": float(score),\n",
    "            })\n",
    "    return results\n",
    "\n",
    "# --------------- Save & Evaluate (pycocotools) ---------------\n",
    "def run_coco_eval(gt_path: str, dt_path: str):\n",
    "    from pycocotools.coco import COCO\n",
    "    from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "    cocoGt = COCO(gt_path)\n",
    "    cocoDt = cocoGt.loadRes(dt_path) if os.path.getsize(dt_path) > 2 else cocoGt.loadRes([])\n",
    "    img_ids = sorted(cocoGt.getImgIds())\n",
    "\n",
    "    # AP/AR on standard COCO range 0.50:0.95 (for reference)\n",
    "    ev_std = COCOeval(cocoGt, cocoDt, iouType='bbox')\n",
    "    ev_std.params.imgIds = img_ids\n",
    "    ev_std.evaluate(); ev_std.accumulate(); ev_std.summarize()\n",
    "    ap_std = ev_std.stats[0]  # AP@[.50:.95]\n",
    "\n",
    "    # AP@0.50\n",
    "    ev_50 = COCOeval(cocoGt, cocoDt, iouType='bbox')\n",
    "    ev_50.params.imgIds = img_ids\n",
    "    ev_50.params.iouThrs = np.array([0.50])\n",
    "    ev_50.evaluate(); ev_50.accumulate(); ev_50.summarize()\n",
    "    ap50 = ev_50.stats[0]  # AP at single IoU\n",
    "\n",
    "    # AP@0.75\n",
    "    ev_75 = COCOeval(cocoGt, cocoDt, iouType='bbox')\n",
    "    ev_75.params.imgIds = img_ids\n",
    "    ev_75.params.iouThrs = np.array([0.75])\n",
    "    ev_75.evaluate(); ev_75.accumulate(); ev_75.summarize()\n",
    "    ap75 = ev_75.stats[0]\n",
    "\n",
    "    # AP@[0.50:0.90] step 0.05\n",
    "    ev_5090 = COCOeval(cocoGt, cocoDt, iouType='bbox')\n",
    "    ev_5090.params.imgIds = img_ids\n",
    "    ev_5090.params.iouThrs = np.arange(0.50, 0.95, 0.05)  # 0.50,0.55,...,0.90\n",
    "    ev_5090.evaluate(); ev_5090.accumulate(); ev_5090.summarize()\n",
    "    ap_5090 = ev_5090.stats[0]\n",
    "\n",
    "    # AR@0.50\n",
    "    ev_ar50 = COCOeval(cocoGt, cocoDt, iouType='bbox')\n",
    "    ev_ar50.params.imgIds = img_ids\n",
    "    ev_ar50.params.iouThrs = np.array([0.50])\n",
    "    ev_ar50.evaluate(); ev_ar50.accumulate(); ev_ar50.summarize()\n",
    "    ar50 = ev_ar50.stats[8]  # AR (area=all, maxDets=100)\n",
    "\n",
    "    # AR@0.75\n",
    "    ev_ar75 = COCOeval(cocoGt, cocoDt, iouType='bbox')\n",
    "    ev_ar75.params.imgIds = img_ids\n",
    "    ev_ar75.params.iouThrs = np.array([0.75])\n",
    "    ev_ar75.evaluate(); ev_ar75.accumulate(); ev_ar75.summarize()\n",
    "    ar75 = ev_ar75.stats[8]\n",
    "\n",
    "    # AR@[0.50:0.90]\n",
    "    ev_ar5090 = COCOeval(cocoGt, cocoDt, iouType='bbox')\n",
    "    ev_ar5090.params.imgIds = img_ids\n",
    "    ev_ar5090.params.iouThrs = np.arange(0.50, 0.95, 0.05)\n",
    "    ev_ar5090.evaluate(); ev_ar5090.accumulate(); ev_ar5090.summarize()\n",
    "    ar_5090 = ev_ar5090.stats[8]\n",
    "\n",
    "    print(\"\\n======= Custom Summary =======\")\n",
    "    print(f\"AP@[0.50:0.95] : {ap_std:.4f}\")\n",
    "    print(f\"AP@0.50        : {ap50:.4f}\")\n",
    "    print(f\"AP@0.75        : {ap75:.4f}\")\n",
    "    print(f\"AP@[0.50:0.90] : {ap_5090:.4f}\")\n",
    "    print(f\"AR@0.50        : {ar50:.4f}\")\n",
    "    print(f\"AR@0.75        : {ar75:.4f}\")\n",
    "    print(f\"AR@[0.50:0.90] : {ar_5090:.4f}\")\n",
    "    print(\"================================\")\n",
    "\n",
    "    metrics = {\n",
    "        \"AP@[0.50:0.95]\": ap_std,\n",
    "        \"AP@0.50\": ap50,\n",
    "        \"AP@0.75\": ap75,\n",
    "        \"AP@[0.50:0.90]\": ap_5090,\n",
    "        \"AR@0.50\": ar50,\n",
    "        \"AR@0.75\": ar75,\n",
    "        \"AR@[0.50:0.90]\": ar_5090,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# --------------- Pipeline ---------------\n",
    "# 1) Build COCO GT from GT_DIR (mapping names by dropping trailing '_output')\n",
    "coco_gt = build_coco_gt_from_gt_folder(GT_DIR, category_id=CATEGORY_ID, category_name=CATEGORY_NAME)\n",
    "write_json(coco_gt, COCO_GT_PATH)\n",
    "print(f\"Wrote COCO GT to: {COCO_GT_PATH}  | images={len(coco_gt['images'])}, anns={len(coco_gt['annotations'])}\")\n",
    "\n",
    "# 2) Build COCO results from EXP_DIR keyed by coco_gt['images'].file_name\n",
    "coco_dt = build_coco_results_from_exp_folder(coco_gt, EXP_DIR, category_id=CATEGORY_ID)\n",
    "write_json(coco_dt, COCO_DT_PATH)\n",
    "try:\n",
    "    print(f\"Wrote COCO Results to: {COCO_DT_PATH}  | detections={len(coco_dt)} | images={len(coco_dt['images'])}\")\n",
    "except:\n",
    "    pass\n",
    "# 3) Install pycocotools (if not already) and evaluate\n",
    "try:\n",
    "    from pycocotools.coco import COCO  # noqa: F401\n",
    "except Exception:\n",
    "    !pip -q install pycocotools\n",
    "\n",
    "metrics = run_coco_eval(COCO_GT_PATH, COCO_DT_PATH)\n",
    "\n",
    "all_results[\"skip 0\"] = [\n",
    "    metrics[\"AP@0.50\"],\n",
    "    metrics[\"AP@0.75\"],\n",
    "    metrics[\"AP@[0.50:0.95]\"],\n",
    "    metrics[\"AP@[0.50:0.90]\"],\n",
    "    metrics[\"AR@0.50\"],\n",
    "    metrics[\"AR@0.75\"],\n",
    "    metrics[\"AR@[0.50:0.90]\"],\n",
    "]\n",
    "\n",
    "print(\"\\nUpdated results dict:\")\n",
    "print(all_results)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8144570,
     "sourceId": 13221078,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
