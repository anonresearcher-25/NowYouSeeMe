{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Use the following commands to update your system, install `ffmpeg`, upgrade `pip`, and install all the necessary Python dependencies.\n",
    "\n",
    "### 1. Update and Install `ffmpeg`\n",
    "\n",
    "```bash\n",
    "!apt-get -y update && apt-get -y install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-06T10:52:07.312972Z",
     "iopub.status.busy": "2025-09-06T10:52:07.312558Z",
     "iopub.status.idle": "2025-09-06T10:54:09.702263Z",
     "shell.execute_reply": "2025-09-06T10:54:09.700594Z",
     "shell.execute_reply.started": "2025-09-06T10:52:07.312914Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!apt-get -y update && apt-get -y install ffmpeg\n",
    "!python -m pip install --upgrade pip\n",
    "!pip install \"opencv-python==4.11.0.86\" \"numpy==1.26.4\" \\\n",
    "             \"pycryptodome==3.20.0\" \"onnxruntime==1.18.1\" \\\n",
    "             \"timm==1.0.9\" \"rich==14.1.0\"\n",
    "# Torch and TF are preinstalled on Kaggle; you can pin if you need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup Script\n",
    "\n",
    "This script sets up the working environment for a project by copying a repository, preparing directories, and validating data paths.  \n",
    "It ensures that all necessary components (repo, videos, and CSV index) are available before proceeding.\n",
    "\n",
    "---\n",
    "\n",
    "### 📁 1. Import Required Modules\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T10:57:58.125162Z",
     "iopub.status.busy": "2025-09-06T10:57:58.124394Z",
     "iopub.status.idle": "2025-09-06T10:57:58.270200Z",
     "shell.execute_reply": "2025-09-06T10:57:58.269165Z",
     "shell.execute_reply.started": "2025-09-06T10:57:58.125130Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "# Define project structure (replace with your actual dataset/repo names)\n",
    "DATASET_ROOT = Path(\"/path/to/dataset\")\n",
    "WORK_ROOT = Path(\"/path/to/working_dir\")\n",
    "\n",
    "REPO_NAME = \"YourRepoName\"\n",
    "REPO_SRC = DATASET_ROOT / REPO_NAME\n",
    "REPO_DST = WORK_ROOT / REPO_NAME\n",
    "\n",
    "# Copy the repo into working directory so it can be imported/edited freely\n",
    "if REPO_DST.exists():\n",
    "    shutil.rmtree(REPO_DST)\n",
    "shutil.copytree(REPO_SRC, REPO_DST)\n",
    "\n",
    "# Add repo to Python path\n",
    "sys.path.append(str(REPO_DST))\n",
    "\n",
    "# Outputs\n",
    "OUT_ROOT = WORK_ROOT / \"output\"\n",
    "OUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Data files (adjust to your dataset layout)\n",
    "VIDEOS_DIR = DATASET_ROOT / \"videos\"\n",
    "CSV_PATH = DATASET_ROOT / \"video_index.csv\"\n",
    "\n",
    "# Sanity checks\n",
    "assert REPO_DST.exists(), \"Repo not found\"\n",
    "assert VIDEOS_DIR.exists(), \"Videos directory missing\"\n",
    "assert CSV_PATH.exists(), \"CSV file missing\"\n",
    "\n",
    "# Info logs\n",
    "print(f\"Repo:   {REPO_DST}\")\n",
    "print(f\"Videos: {len(list(VIDEOS_DIR.glob('*.mov')))}\")\n",
    "print(f\"CSV:    {CSV_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Environment Configuration\n",
    "\n",
    "This script sets important environment variables to control **TensorFlow**, **JAX**, and **XLA** behavior.  \n",
    "It’s especially useful in development environments (e.g., Kaggle, Colab, or local CPU-only runs) where you want to **suppress logs**, **force CPU usage**, or **avoid unnecessary GPU allocation**.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 1. Configure Environment Variables\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "# Suppress TensorFlow logs (INFO, WARNING, and ERROR)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# Disable GPU usage for TensorFlow and JAX\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "# Force JAX to run on CPU only\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n",
    "\n",
    "# Prevent XLA from preallocating large GPU memory chunks\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\n",
    "\n",
    "# Disable oneDNN optimization logs to reduce console noise\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T10:58:21.276881Z",
     "iopub.status.busy": "2025-09-06T10:58:21.276454Z",
     "iopub.status.idle": "2025-09-06T10:58:21.283780Z",
     "shell.execute_reply": "2025-09-06T10:58:21.282238Z",
     "shell.execute_reply.started": "2025-09-06T10:58:21.276850Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"       # hide TF INFO/WARN/ERROR\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"        # no GPU for TF/JAX\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"cpu\"            # JAX stays on CPU\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"  # don't grab big GPU chunks\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"      # avoid extra kernel logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔐 AES Encryption & Video Processing Utilities\n",
    "\n",
    "This script provides a set of **utility functions** for secure data handling, video preprocessing, and frame selection.  \n",
    "It’s designed for machine learning pipelines that process video frames, encrypt sensitive metadata, and handle orientation and target frame extraction.\n",
    "\n",
    "---\n",
    "\n",
    "### 📦 1. Imports & Dependencies\n",
    "\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "from Crypto.Cipher import AES, PKCS1_OAEP\n",
    "from Crypto.Random import get_random_bytes\n",
    "from Crypto.PublicKey import RSA\n",
    "import base64, json, numpy as np, cv2, time, subprocess, torch, csv, ast, shlex, uuid, os\n",
    "from pathlib import Path\n",
    "\n",
    "import src.blur_functions as bf\n",
    "import src.utils as utils\n",
    "from src.backbones import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T10:58:21.287343Z",
     "iopub.status.busy": "2025-09-06T10:58:21.285698Z",
     "iopub.status.idle": "2025-09-06T10:58:21.319550Z",
     "shell.execute_reply": "2025-09-06T10:58:21.317451Z",
     "shell.execute_reply.started": "2025-09-06T10:58:21.287300Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from Crypto.Cipher import AES\n",
    "from Crypto.Random import get_random_bytes\n",
    "from Crypto.Cipher import PKCS1_OAEP\n",
    "from Crypto.PublicKey import RSA\n",
    "import base64, json, numpy as np, cv2, time, subprocess, torch, csv, ast, shlex, uuid, os\n",
    "from pathlib import Path\n",
    "\n",
    "import src.blur_functions as bf\n",
    "import src.utils as utils\n",
    "from src.backbones import get_model\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"   # CPU; remove if you want GPU for embeddings\n",
    "\n",
    "def box_center(box):\n",
    "    x1, y1, x2, y2 = box\n",
    "    return ((x1 + x2) // 2, (y1 + y2) // 2)\n",
    "\n",
    "def pad(data: bytes):\n",
    "    pad_len = 16 - (len(data) % 16)\n",
    "    return data + bytes([pad_len]) * pad_len\n",
    "\n",
    "def encrypt_data_aes128(data: bytes, key: bytes) -> str:\n",
    "    cipher = AES.new(key, AES.MODE_ECB)\n",
    "    return base64.b64encode(cipher.encrypt(pad(data))).decode(\"utf-8\")\n",
    "\n",
    "def _parse_frame_list_cell(cell):\n",
    "    if cell is None: return []\n",
    "    if isinstance(cell, list): return [int(x) for x in cell]\n",
    "    s = str(cell).strip()\n",
    "    if not s: return []\n",
    "    try:\n",
    "        val = ast.literal_eval(s)\n",
    "        if isinstance(val, (list, tuple)): return [int(x) for x in val]\n",
    "    except Exception:\n",
    "        pass\n",
    "    out = []\n",
    "    for p in s.strip(\"[]\").split(\",\"):\n",
    "        p = p.strip()\n",
    "        if p and p.lstrip(\"-\").isdigit(): out.append(int(p))\n",
    "    return out\n",
    "\n",
    "def load_target_frames_for_video(csv_path, video_name_key):\n",
    "    target = set()\n",
    "    with open(csv_path, \"r\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            v = row.get(\"video_name\", \"\")\n",
    "            if Path(v).stem == Path(video_name_key).stem:\n",
    "                for idx in _parse_frame_list_cell(row.get(\"frame_numbers\", \"\")):\n",
    "                    target.add(int(idx))\n",
    "    return target\n",
    "\n",
    "def probe_rotation_deg(path) -> int:\n",
    "    try:\n",
    "        out = subprocess.check_output(\n",
    "            shlex.split(\n",
    "                f'ffprobe -v error -select_streams v:0 -show_entries stream_tags=rotate -of json \"{path}\"'\n",
    "            )\n",
    "        ).decode(\"utf-8\")\n",
    "        data = json.loads(out)\n",
    "        rotate = int(data.get(\"streams\", [{}])[0].get(\"tags\", {}).get(\"rotate\", \"0\"))\n",
    "        return rotate % 360\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "def rotate_fixed(img, deg):\n",
    "    if deg == 90:  return cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "    if deg == 180: return cv2.rotate(img, cv2.ROTATE_180)\n",
    "    if deg == 270: return cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ Configuration: Video Processing Pipeline\n",
    "\n",
    "This script defines a `Config` class that centralizes **all key parameters** for your video processing pipeline — including input/output paths, model locations, runtime options, and detection thresholds.  \n",
    "It also sets up public key loading for encryption.\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ 1. Configuration Class\n",
    "\n",
    "```python\n",
    "class Config:\n",
    "    # ▶️ Input / Output Settings\n",
    "    INPUT_VIDEO_PATH   = str(VIDEOS_DIR / \"YourVideoName.mp4\")\n",
    "    OUTPUT_VIDEO_PATH  = str(OUT_ROOT / \"single\" / \"blurred.mp4\")\n",
    "    SAVE_OUTPUT        = False         # Save processed video\n",
    "    AUDIO              = False         # Include audio in output\n",
    "\n",
    "    # 🤖 Models\n",
    "    DETECTOR_MODEL     = \"yunet\"       # Face detector model\n",
    "    EMBEDDING_MODEL    = \"edgeface_s_gamma_05\"\n",
    "    EMBEDDING_MODEL_PATH = str(REPO_NAME / \"src\" / \"models\" / \"edgeface_s_gamma_05.pt\")\n",
    "\n",
    "    # ⏱️ Frame Processing\n",
    "    FRAME_SKIP         = 3             # Process every 3rd frame\n",
    "    TRACKING_SKIP      = 0             # Tracking interval\n",
    "    BLUR_ENABLED       = True          # Enable face blurring\n",
    "\n",
    "    # 🔄 Video Orientation\n",
    "    ROTATION_DEGREES   = 0             # 0 => auto from metadata; use 90/180/270 to override\n",
    "\n",
    "    # 📍 Tracking Thresholds\n",
    "    LANDMARK_STALENESS_THRESHOLD = 15\n",
    "    BOX_STALENESS_THRESHOLD      = 15\n",
    "    MSE_THRESHOLD                = 150\n",
    "\n",
    "    # 📦 Detector Settings\n",
    "    SCRFD_MODEL_PATH   = str(REPO_NAME / \"src\" / \"models\" / \"det_500m.onnx\")\n",
    "    SCRFD_INPUT_SIZE   = (640, 640)\n",
    "\n",
    "    # 🎨 Visualization Options\n",
    "    OVERLAY_LANDMARKS     = False\n",
    "    OVERLAY_DETECTOR_BOX  = False\n",
    "    DISPLAY_VIDEO         = False\n",
    "\n",
    "    # 📊 Frame Metadata\n",
    "    FRAMES_CSV_PATH   = str(CSV_PATH)\n",
    "    FRAME_JSON_DIR    = str(OUT_ROOT / \"single\" / \"frame_json\")\n",
    "    # FRAME_IMAGE_DIR   = str(OUT_ROOT / \"single\" / \"frame_images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T10:58:21.321577Z",
     "iopub.status.busy": "2025-09-06T10:58:21.321052Z",
     "iopub.status.idle": "2025-09-06T10:58:21.371108Z",
     "shell.execute_reply": "2025-09-06T10:58:21.369637Z",
     "shell.execute_reply.started": "2025-09-06T10:58:21.321541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # default single-video (overridden when batching)\n",
    "    INPUT_VIDEO_PATH   = str(VIDEOS_DIR / \"YourVideoName.mp4\")\n",
    "    OUTPUT_VIDEO_PATH  = str(OUT_ROOT / \"single\" / \"blurred.mp4\")\n",
    "    SAVE_OUTPUT        = False\n",
    "    AUDIO              = False\n",
    "\n",
    "    DETECTOR_MODEL     = \"yunet\"\n",
    "    EMBEDDING_MODEL    = \"edgeface_s_gamma_05\"\n",
    "    EMBEDDING_MODEL_PATH = str(REPO_NAME / \"src\" / \"models\" / \"edgeface_s_gamma_05.pt\")\n",
    "\n",
    "    FRAME_SKIP         = 3\n",
    "    TRACKING_SKIP      = 0\n",
    "    BLUR_ENABLED       = True\n",
    "\n",
    "    # 0 => use ffprobe metadata; use 90/180/270 to force\n",
    "    ROTATION_DEGREES   = 0\n",
    "\n",
    "    LANDMARK_STALENESS_THRESHOLD = 15\n",
    "    BOX_STALENESS_THRESHOLD      = 15\n",
    "    MSE_THRESHOLD                = 150\n",
    "\n",
    "    SCRFD_MODEL_PATH   = str(REPO_NAME / \"src\" / \"models\" / \"det_500m.onnx\")\n",
    "    SCRFD_INPUT_SIZE   = (640, 640)\n",
    "\n",
    "    OVERLAY_LANDMARKS = False\n",
    "    OVERLAY_DETECTOR_BOX = False\n",
    "    DISPLAY_VIDEO     = False\n",
    "\n",
    "    FRAMES_CSV_PATH   = str(CSV_PATH)\n",
    "    FRAME_JSON_DIR    = str(OUT_ROOT / \"single\" / \"frame_json\")\n",
    "    # FRAME_IMAGE_DIR   = str(OUT_ROOT / \"single\" / \"frame_images\")\n",
    "\n",
    "PUBLIC_KEY_PATH = REPO_NAME / \"encryption\" / \"public_key.pem\"\n",
    "assert PUBLIC_KEY_PATH.exists(), \"public_key.pem not found\"\n",
    "public_key = RSA.import_key(open(PUBLIC_KEY_PATH, \"rb\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Pipeline Initialization & Model Loading\n",
    "\n",
    "This code is the **initial setup phase** of the face-processing pipeline.  \n",
    "It prepares the environment by creating necessary directories, initializing detection and landmark models, and loading a face embedding network.  \n",
    "After this step, the pipeline is ready to process video frames for detection, landmark extraction, blurring, or embedding tasks.\n",
    "\n",
    "---\n",
    "\n",
    "### 🗂️ 1. Create Required Output Directories\n",
    "\n",
    "```python\n",
    "config = Config()\n",
    "\n",
    "# make sure dirs exist for the current config\n",
    "Path(config.FRAME_JSON_DIR).mkdir(parents=True, exist_ok=True)\n",
    "# Path(config.FRAME_IMAGE_DIR).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-06T10:58:21.374258Z",
     "iopub.status.busy": "2025-09-06T10:58:21.373737Z",
     "iopub.status.idle": "2025-09-06T10:58:21.710728Z",
     "shell.execute_reply": "2025-09-06T10:58:21.709311Z",
     "shell.execute_reply.started": "2025-09-06T10:58:21.374229Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# make sure dirs exist for the current config\n",
    "Path(config.FRAME_JSON_DIR).mkdir(parents=True, exist_ok=True)\n",
    "# Path(config.FRAME_IMAGE_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "detector = utils.initialize_detector(config)\n",
    "interpretor, input_details, output_details = utils.initialize_landmark_detector()\n",
    "\n",
    "if config.EMBEDDING_MODEL == \"edgeface_s_gamma_05\":\n",
    "    model_name = \"edgeface_s_gamma_05\"\n",
    "    model = get_model(model_name)\n",
    "    try:\n",
    "        state = torch.load(config.EMBEDDING_MODEL_PATH, map_location='cpu', weights_only=True)\n",
    "        model.load_state_dict(state)\n",
    "    except TypeError:\n",
    "        model.load_state_dict(torch.load(config.EMBEDDING_MODEL_PATH, map_location='cpu'))\n",
    "    model.eval()\n",
    "else:\n",
    "    model = None  # extend if you support other backbones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎥 `process_video_one(cfg: Config)`\n",
    "\n",
    "The `process_video_one()` function is the **main driver** of the video anonymization and face-processing pipeline.  \n",
    "It reads a video frame-by-frame, detects and tracks faces, extracts landmarks, blurs sensitive regions, encrypts metadata, and optionally generates embeddings — all while logging performance statistics.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Overview of What It Does\n",
    "\n",
    "| Stage | Purpose |\n",
    "|-------|---------|\n",
    "| 🧩 Initialization | Prepare variables, trackers, and video I/O |\n",
    "| 🎞️ Frame Processing Loop | Read and process each video frame |\n",
    "| 🔍 Face Detection & Tracking | Detect faces or track them across frames |\n",
    "| 📍 Landmark Extraction | Get facial landmarks for each face |\n",
    "| 🫣 Blurring & Metadata | Blur detected faces and save metadata |\n",
    "| 🔐 Encryption & Storage | Encrypt face data and embeddings |\n",
    "| 📊 Statistics & Logging | Compute performance metrics and save logs |\n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ Initialization & Setup\n",
    "\n",
    "```python\n",
    "start_process_time = time.perf_counter()\n",
    "face_metadata = {}\n",
    "face_tracks = {}\n",
    "face_id_to_aes_key = {}\n",
    "MAX_CENTER_DIST = 60\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2,\n",
    "                 criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def process_video_one(cfg: Config):\n",
    "    start_process_time = time.perf_counter()\n",
    "    total_inference_times = []\n",
    "    total_blur_times = []; total_detector_times=[]; total_landmarker_times=[]\n",
    "    total_tracking_times=[]; total_mse_times=[]\n",
    "    prev_detected_face_scores = [] \n",
    "    store_scores = []\n",
    "\n",
    "    # per-run trackers\n",
    "    face_metadata = {}\n",
    "    face_tracks = {}\n",
    "    face_id_to_aes_key = {}\n",
    "    MAX_CENTER_DIST = 60\n",
    "    lk_params = dict(winSize=(15, 15), maxLevel=2,\n",
    "                     criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "    current_video_name = Path(cfg.INPUT_VIDEO_PATH).stem\n",
    "    TARGET_FRAMES = load_target_frames_for_video(cfg.FRAMES_CSV_PATH, current_video_name)\n",
    "\n",
    "    # rotation\n",
    "    meta_deg = probe_rotation_deg(cfg.INPUT_VIDEO_PATH)\n",
    "    effective_deg = cfg.ROTATION_DEGREES if cfg.ROTATION_DEGREES in (90,180,270) else meta_deg\n",
    "    print(f\"[{current_video_name}] rotation = {effective_deg}° (meta={meta_deg}°)\")\n",
    "\n",
    "    # open IO\n",
    "    cap, out, _, _ = utils.initialize_video_io(cfg)\n",
    "    if not cap or not cap.isOpened():\n",
    "        raise RuntimeError(f\"Failed to open video: {cfg.INPUT_VIDEO_PATH}\")\n",
    "\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)); h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)); fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    print(f\"[{current_video_name}] props: {w}x{h} @{fps:.2f}\")\n",
    "\n",
    "    frame_count = 0; blur_count=0; detector_count=0; landmarker_count=0; tracking_count=0; mse_thresh_count=0\n",
    "    detected_faces_boxes = []; last_landmark_boxes=[]; prev_frame=None; prev_gray=None\n",
    "    frames_since_last_box = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        detected_face_scores = []\n",
    "        frame_start_time = time.perf_counter()\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "\n",
    "        frame_count += 1\n",
    "        frame_idx_zero = frame_count - 1\n",
    "\n",
    "        frame = rotate_fixed(frame, effective_deg)\n",
    "        frame_blur_records = []\n",
    "\n",
    "        # MSE motion gating\n",
    "        mse_values = []\n",
    "        if prev_frame is not None and detected_faces_boxes:\n",
    "            for face_region in detected_faces_boxes:\n",
    "                st = time.perf_counter()\n",
    "                mse = utils.calculate_frame_diff(prev_frame, frame, face_region)\n",
    "                en = time.perf_counter()\n",
    "                total_mse_times.append((en-st)*1000)\n",
    "                mse_values.append(mse)\n",
    "\n",
    "        run_detector = (frame_count == 1\n",
    "                        or frame_count % (cfg.FRAME_SKIP + 1) == 0\n",
    "                        or (mse_values and max(mse_values) > cfg.MSE_THRESHOLD))\n",
    "        if (mse_values and max(mse_values) > cfg.MSE_THRESHOLD):\n",
    "            mse_thresh_count += 1\n",
    "\n",
    "        output_frame = frame\n",
    "        rgb_frame = output_frame\n",
    "        prev_frame = output_frame\n",
    "        current_height, current_width = frame.shape[:2]\n",
    "        detected_face_scores = []\n",
    "\n",
    "        if run_detector:\n",
    "            detector_count += 1\n",
    "            st = time.perf_counter()\n",
    "            faces = utils.detect_faces(rgb_frame, detector, cfg)\n",
    "            #print(\"faces\", faces)\n",
    "            en = time.perf_counter(); total_detector_times.append((en-st)*1000)\n",
    "            detected_faces_boxes = [utils.expand_box_with_margin(f[\"box\"], 0.1, output_frame.shape[1], output_frame.shape[0]) for f in faces]\n",
    "            detected_face_scores = [f[\"score\"] for f in faces]\n",
    "            store_scores = copy.deepcopy(detected_face_scores)\n",
    "            #print(\"detected faces\", detected_face_scores)\n",
    "            if detected_faces_boxes:\n",
    "                frames_since_last_box = 0\n",
    "            else:\n",
    "                frames_since_last_box += 1\n",
    "                if frames_since_last_box >= cfg.BOX_STALENESS_THRESHOLD:\n",
    "                    detected_faces_boxes = []\n",
    "                face_tracks = {}\n",
    "            prev_detected_face_scores = store_scores\n",
    "        else:\n",
    "           # if not detected_face_scores and prev_detected_face_scores:\n",
    "             #   detected_face_scores = prev_detected_face_scores\n",
    "            frames_since_last_box += 1\n",
    "            if frames_since_last_box >= cfg.BOX_STALENESS_THRESHOLD:\n",
    "                detected_faces_boxes = []\n",
    "            if prev_gray is not None and (frame_count % (cfg.TRACKING_SKIP+1) == 0):\n",
    "                st = time.perf_counter()\n",
    "                detected_faces_boxes = [utils.track_box_with_optical_flow(prev_gray, rgb_frame, box, lk_params, scale_factor=0.4)[0]\n",
    "                                        for box in detected_faces_boxes]\n",
    "                en = time.perf_counter(); tracking_count += 1; total_tracking_times.append((en-st)*1000)\n",
    "\n",
    "        new_tracks = {}\n",
    "        for i, box in enumerate(detected_faces_boxes):\n",
    "            cx, cy = box_center(box)\n",
    "            matched_id = None; min_dist = float(\"inf\")\n",
    "            for face_id, info in face_tracks.items():\n",
    "                pcx, pcy = box_center(info[\"box\"])\n",
    "                dist = np.linalg.norm(np.array([cx, cy]) - np.array([pcx, pcy]))\n",
    "                if dist < MAX_CENTER_DIST and dist < min_dist:\n",
    "                    matched_id = face_id; min_dist = dist\n",
    "\n",
    "            if matched_id:\n",
    "                new_tracks[matched_id] = {\"box\": box, \"aes_key\": face_tracks[matched_id][\"aes_key\"]}\n",
    "            else:\n",
    "                new_id = str(uuid.uuid4()); new_key = get_random_bytes(16)\n",
    "                face_id_to_aes_key[new_id] = new_key\n",
    "                new_tracks[new_id] = {\"box\": box, \"aes_key\": new_key}\n",
    "\n",
    "            x1, y1, x2, y2 = box\n",
    "            if x1 < x2 and y1 < y2:\n",
    "                x1=max(x1,0); y1=max(y1,0); x2=min(x2,current_width); y2=min(y2,current_height)\n",
    "                face_roi_rgb = rgb_frame[y1:y2, x1:x2]\n",
    "                if face_roi_rgb.size > 0:\n",
    "                    st = time.perf_counter()\n",
    "                    landmarks, score = utils.get_landmarks_interpretor_with_score(face_roi_rgb, interpretor, input_details, output_details)\n",
    "                    en = time.perf_counter(); total_landmarker_times.append((en-st)*1000); landmarker_count += 1\n",
    "\n",
    "                    landmarks_scaled = utils.scale_landmarks(landmarks, x1, y1, x2-x1, y2-y1)\n",
    "\n",
    "                    face_id = matched_id if matched_id else new_id\n",
    "                    track_info = new_tracks[face_id]; aes_key = track_info[\"aes_key\"]\n",
    "\n",
    "                    frame_h, frame_w = frame.shape[:2]\n",
    "                    hull = cv2.convexHull(landmarks_scaled)\n",
    "                    hull_pts = np.array(hull).reshape(-1,2).astype(int).tolist()\n",
    "\n",
    "                    # base bounding rect\n",
    "                    x, y, w, h = cv2.boundingRect(hull)\n",
    "\n",
    "                    # scale box 1.15x\n",
    "                    scale = 1.0\n",
    "                    cx, cy = x + w/2, y + h/2\n",
    "                    new_w, new_h = w * scale, h * scale\n",
    "                    x1 = int(max(0, cx - new_w/2))\n",
    "                    y1 = int(max(0, cy - new_h/2))\n",
    "                    x2 = int(min(frame_w, cx + new_w/2))\n",
    "                    y2 = int(min(frame_h, cy + new_h/2))\n",
    "\n",
    "                    # --- discard very small faces (< 30x30 px²) ---\n",
    "                    box_area = (x2 - x1) * (y2 - y1)\n",
    "                    if box_area < 30 * 30:\n",
    "                        continue\n",
    "\n",
    "                    # save record\n",
    "                    # choose detection score with fallbacks\n",
    "                    if i < len(store_scores):\n",
    "                        det_score_val = float(store_scores[i])\n",
    "                    elif len(detected_face_scores) > 0:\n",
    "                        det_score_val = float(store_scores[-1])\n",
    "                    elif prev_detected_face_scores:\n",
    "                        det_score_val = float(prev_detected_face_scores[-1])\n",
    "                    else:\n",
    "                        det_score_val = 0.5\n",
    "                        print(\"random\")\n",
    "                    \n",
    "                    frame_blur_records.append({\n",
    "                        \"face_id\": face_id,\n",
    "                        \"pred_bbox\": [x1, y1, x2, y2],\n",
    "                        \"hull\": hull_pts,\n",
    "                        \"det_score\": det_score_val,\n",
    "                        \"land_score\": float(score)\n",
    "                    })\n",
    "                    #print(\"score\", i, len(detected_face_scores), \"→\", det_score_val)\n",
    "\n",
    "                    # adjust landmarks relative to cropped box\n",
    "                    landmarks_adjusted = np.array(landmarks_scaled)\n",
    "                    landmarks_adjusted[:,0] -= x1\n",
    "                    landmarks_adjusted[:,1] -= y1\n",
    "\n",
    "                    # crop face image\n",
    "                    face_image_for_encryption = rgb_frame[y1:y2, x1:x2]\n",
    "                    _, buffer = cv2.imencode('.png', face_image_for_encryption)\n",
    "                    face_image_encoded = base64.b64encode(buffer).decode('utf-8')\n",
    "\n",
    "                    data = {\n",
    "                        \"frame\": frame_count,\n",
    "                        \"landmarks\": landmarks_adjusted.tolist(),\n",
    "                        \"face_image\": face_image_encoded,\n",
    "                        \"box\": [x1, y1, x2, y2],\n",
    "                        \"box_score\": float(detected_face_scores[i]) if i < len(detected_face_scores) else float(\"nan\"),\n",
    "                        \"land_score\": float(score),\n",
    "                        \"face_id\": face_id\n",
    "                    }\n",
    "                    face_metadata.setdefault(face_id, []).append(data)\n",
    "\n",
    "                    # blur if enabled\n",
    "                    if cfg.BLUR_ENABLED:\n",
    "                        st=time.perf_counter()\n",
    "                        output_frame = bf.apply_blur_new(rgb_frame, hull, x1, y1, x2-x1, y2-y1)\n",
    "                        en=time.perf_counter(); total_blur_times.append((en-st)*1000); blur_count+=1\n",
    "\n",
    "        face_tracks = new_tracks\n",
    "        prev_gray = output_frame\n",
    "\n",
    "        # save full-video out if requested\n",
    "        if cfg.SAVE_OUTPUT and out: out.write(output_frame)\n",
    "\n",
    "        # save per-frame artifacts only for CSV target frames\n",
    "        if frame_idx_zero in TARGET_FRAMES:\n",
    "            base = f\"{current_video_name}_frame{frame_idx_zero}\"\n",
    "            json_out_path = Path(cfg.FRAME_JSON_DIR) / f\"{base}.json\"\n",
    "            json_out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            with open(json_out_path, \"w\", encoding=\"utf-8\") as jf:\n",
    "                json.dump(frame_blur_records, jf, indent=2)\n",
    "            #img_out_path  = Path(cfg.FRAME_IMAGE_DIR)  / f\"{base}.png\"\n",
    "            #cv2.imwrite(str(img_out_path), output_frame)\n",
    "\n",
    "        frame_end_time = time.perf_counter()\n",
    "        total_inference_times.append((frame_end_time - frame_start_time)*1000)\n",
    "\n",
    "    # cleanup\n",
    "    cap.release()\n",
    "    if out: out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # postprocess encryption + embeddings\n",
    "    encrypted_face_metadata = {}; to_be_embedded = {}\n",
    "    for face_id, entries in face_metadata.items():\n",
    "        aes_key = face_id_to_aes_key[face_id]\n",
    "        encrypted_entries = []\n",
    "        best_idx = max(range(len(entries)), key=lambda k: entries[k].get(\"land_score\", -1))\n",
    "        for entry in entries:\n",
    "            encrypted_entries.append(encrypt_data_aes128(json.dumps(entry).encode(\"utf-8\"), aes_key))\n",
    "        encrypted_face_metadata[f\"frame_{face_id}\"] = encrypted_entries\n",
    "        to_be_embedded[face_id] = (entries[best_idx].get(\"face_image\"), entries[best_idx].get(\"landmarks\"))\n",
    "\n",
    "    (OUT_ROOT/\"video_metadata_encrypted.json\").write_text(json.dumps(encrypted_face_metadata, indent=2))\n",
    "\n",
    "    if to_be_embedded and (model is not None):\n",
    "        batch_tensor, face_id_order = utils.preprocess_face_images(to_be_embedded, config.EMBEDDING_MODEL)\n",
    "        if batch_tensor is not None:\n",
    "            with torch.no_grad():\n",
    "                embeddings = model(batch_tensor)\n",
    "            face_embeddings = embeddings.numpy().tolist()\n",
    "            cipher_rsa = PKCS1_OAEP.new(public_key)\n",
    "            face_id_to_combined_key_embedding = {}\n",
    "            for i, face_id in enumerate(face_id_order):\n",
    "                aes_key = face_id_to_aes_key[face_id]\n",
    "                encrypted_aes_key_b64 = base64.b64encode(cipher_rsa.encrypt(aes_key)).decode('utf-8')\n",
    "                encrypted_embedding_b64 = encrypt_data_aes128(json.dumps(face_embeddings[i]).encode('utf-8'), aes_key)\n",
    "                face_id_to_combined_key_embedding[face_id] = {\n",
    "                    \"aes_key\": encrypted_aes_key_b64,\n",
    "                    \"embedding\": encrypted_embedding_b64\n",
    "                }\n",
    "            face_id_to_combined_key_embedding[\"video_identifier\"] = str(uuid.uuid4())\n",
    "            (OUT_ROOT/\"face_keys.json\").write_text(json.dumps(face_id_to_combined_key_embedding, indent=2))\n",
    "        else:\n",
    "            (OUT_ROOT/\"face_keys.json\").write_text(json.dumps({\"video_identifier\": str(uuid.uuid4())}, indent=2))\n",
    "    else:\n",
    "        (OUT_ROOT/\"face_keys.json\").write_text(json.dumps({\"video_identifier\": str(uuid.uuid4())}, indent=2))\n",
    "\n",
    "    # stats\n",
    "        # stats\n",
    "    if total_inference_times:\n",
    "        avg_inf = sum(total_inference_times)/len(total_inference_times)\n",
    "        avg_fps = 1000/avg_inf if avg_inf>0 else 0\n",
    "        print(f\"[{current_video_name}] frames: {frame_count}  avg_ms: {avg_inf:.2f}  fps~{avg_fps:.2f}\")\n",
    "\n",
    "    # --- log total blurring time ---\n",
    "    total_blurring_time = sum(total_blur_times)  # in ms\n",
    "    blur_log = {\n",
    "        \"video_name\": str(cfg.INPUT_VIDEO_PATH),\n",
    "        \"total_blurring_time_ms\": total_blurring_time\n",
    "    }\n",
    "    blur_log_path = OUT_ROOT / \"blur_stats.json\"\n",
    "    if blur_log_path.exists():\n",
    "        # append to existing JSON (list of logs)\n",
    "        existing = json.loads(blur_log_path.read_text())\n",
    "        if isinstance(existing, list):\n",
    "            existing.append(blur_log)\n",
    "        else:\n",
    "            existing = [existing, blur_log]\n",
    "        blur_log_path.write_text(json.dumps(existing, indent=2))\n",
    "    else:\n",
    "        blur_log_path.write_text(json.dumps([blur_log], indent=2))\n",
    "\n",
    "    print(\"Done:\", current_video_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "cv2.destroyAllWindows = lambda: None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 Batch Video Processing & Metadata Export\n",
    "\n",
    "This script automates **batch processing** of multiple videos of our dataset.  \n",
    "It iterates over predefined subfolders, runs the full face-detection, landmark, blurring, and encryption pipeline for each video, and exports results (JSON metadata and optionally blurred videos) into organized output directories.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Overview of What It Does\n",
    "\n",
    "| Step | Purpose |\n",
    "|------|---------|\n",
    "| 📁 Folder Iteration | Go through all subfolders (categories) and collect video files |\n",
    "| 🎞️ Per-Video Processing | Run the full `process_video_one()` pipeline on each video |\n",
    "| 📊 Frame Metadata Export | Save per-frame JSON metadata for each video |\n",
    "| 📦 Batch Summary | Count processed videos and total metadata outputs |\n",
    "| 📁 Archiving | Zip all JSON metadata and optionally blurred videos for easy download |\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ 1. Folder & File Configuration\n",
    "\n",
    "```python\n",
    "FOLDERS = [\n",
    "    \"1\", \"2\", \"3\", \"4\", \"5\",\n",
    "    \"Rest\", \"Bystander_Movement\", \"Head_Movement\",\n",
    "    \"Close\", \"Far\", \"Medium\"\n",
    "]\n",
    "\n",
    "VIDEO_PATTERNS = [\"*.mov\", \"*.mp4\"]\n",
    "SAVE_BLURRED_VIDEOS = False  # toggle MP4 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Folders to iterate\n",
    "FOLDERS = [\n",
    "    \"1\", \"2\", \"3\", \"4\", \"5\",\n",
    "    \"Rest\", \"Bystander_Movement\", \"Head_Movement\",\n",
    "    \"Close\", \"Far\", \"Medium\"\n",
    "]\n",
    "\n",
    "VIDEO_PATTERNS = [\"*.mov\", \"*.mp4\"]\n",
    "SAVE_BLURRED_VIDEOS = False  # toggle MP4 output\n",
    "\n",
    "def iter_videos(folder: Path, patterns):\n",
    "    vids = []\n",
    "    for pat in patterns:\n",
    "        vids.extend(folder.glob(pat))\n",
    "    return sorted(vids, key=lambda p: p.name.lower())\n",
    "\n",
    "grand_total_videos = 0\n",
    "grand_total_jsons = 0\n",
    "\n",
    "# One common output folder for all JSONs and (optional) blurred videos\n",
    "json_dir = OUT_ROOT / \"frame_json\"\n",
    "blur_dir = OUT_ROOT / \"blurred\"\n",
    "json_dir.mkdir(parents=True, exist_ok=True)\n",
    "if SAVE_BLURRED_VIDEOS:\n",
    "    blur_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Expecting {len(FOLDERS)} subfolders under {DATASET_ROOT}\")\n",
    "\n",
    "for folder_name in FOLDERS:\n",
    "    src_dir = DATASET_ROOT / str(folder_name)\n",
    "    if not src_dir.exists() or not src_dir.is_dir():\n",
    "        print(f\"\\n=== Folder: {folder_name} | ⚠ not found at {src_dir}\")\n",
    "        continue\n",
    "\n",
    "    videos = iter_videos(src_dir, VIDEO_PATTERNS)\n",
    "    num_videos = len(videos)\n",
    "    print(f\"\\n=== Folder: {folder_name} | {num_videos} video(s) in {src_dir} ===\")\n",
    "\n",
    "    processed_count = 0\n",
    "    folder_json_count = 0\n",
    "\n",
    "    for idx, vpath in enumerate(videos, start=1):\n",
    "        print(f\"\\n--- [{folder_name}] {idx}/{num_videos}: {vpath.name} ---\")\n",
    "\n",
    "        # Point config at common outputs\n",
    "        config.INPUT_VIDEO_PATH = str(vpath)\n",
    "        config.FRAME_JSON_DIR = str(json_dir)\n",
    "        config.SAVE_OUTPUT = bool(SAVE_BLURRED_VIDEOS)\n",
    "        if SAVE_BLURRED_VIDEOS:\n",
    "            config.OUTPUT_VIDEO_PATH = str(blur_dir / f\"{vpath.stem}_blurred.mp4\")\n",
    "\n",
    "        # Run pipeline\n",
    "        try:\n",
    "            process_video_one(config)\n",
    "            processed_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Pipeline error for {vpath.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Count JSONs\n",
    "        saved_jsons = sorted(json_dir.glob(f\"{vpath.stem}_frame*.json\"))\n",
    "        folder_json_count += len(saved_jsons)\n",
    "\n",
    "        # Optional frame check\n",
    "        try:\n",
    "            expected_frames = load_target_frames_for_video(config.FRAMES_CSV_PATH, vpath.stem)\n",
    "        except Exception:\n",
    "            expected_frames = None\n",
    "\n",
    "        if expected_frames is not None:\n",
    "            try:\n",
    "                expected_set = set(int(x) for x in expected_frames)\n",
    "                got_frames = {int(p.stem.split(\"_frame\", 1)[-1]) for p in saved_jsons}\n",
    "                missing = sorted(expected_set - got_frames)\n",
    "                if missing:\n",
    "                    print(f\"Missing {len(missing)} expected frames. Example: {missing[:10]}\")\n",
    "                else:\n",
    "                    print(f\"All {len(got_frames)} expected frames saved.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Frame check error for {vpath.stem}: {e}\")\n",
    "        else:\n",
    "            print(f\"Saved {len(saved_jsons)} JSON files for {vpath.stem}.\")\n",
    "\n",
    "    grand_total_videos += processed_count\n",
    "    grand_total_jsons += folder_json_count\n",
    "\n",
    "    print(f\"\\n>>> Folder complete: {folder_name}\")\n",
    "    print(f\"    Videos processed: {processed_count}/{num_videos}\")\n",
    "    print(f\"    JSONs saved:      {folder_json_count}\")\n",
    "\n",
    "# --- Final zip(s) ---\n",
    "# One big zip with all JSONs\n",
    "zip_jsons = OUT_ROOT / \"all_frame_jsons.zip\"\n",
    "if zip_jsons.exists():\n",
    "    zip_jsons.unlink()\n",
    "shutil.make_archive(str(zip_jsons.with_suffix(\"\")), 'zip', root_dir=json_dir)\n",
    "print(f\"\\nZipped all JSONs → {zip_jsons}\")\n",
    "\n",
    "# (Optional) one zip for blurred videos if enabled\n",
    "if SAVE_BLURRED_VIDEOS:\n",
    "    zip_videos = OUT_ROOT / \"all_blurred_videos.zip\"\n",
    "    if zip_videos.exists():\n",
    "        zip_videos.unlink()\n",
    "    shutil.make_archive(str(zip_videos.with_suffix(\"\")), 'zip', root_dir=blur_dir)\n",
    "    print(f\"Zipped all blurred videos → {zip_videos}\")\n",
    "\n",
    "print(\"\\n======== Batch complete ========\")\n",
    "print(f\"Total videos processed: {grand_total_videos}\")\n",
    "print(f\"Total JSONs saved:      {grand_total_jsons}\")\n",
    "print(\"All outputs organized under:\", OUT_ROOT.resolve())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 COCO Evaluation Pipeline for Face Detection\n",
    "\n",
    "This script converts ground-truth and prediction JSON files into **COCO-compatible format** and runs a full **evaluation** using `pycocotools`.  \n",
    "It calculates standard detection metrics such as **Average Precision (AP)** and **Average Recall (AR)** at different IoU thresholds to evaluate the performance of your pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 What This Script Does\n",
    "\n",
    "| Stage | Purpose |\n",
    "|-------|---------|\n",
    "| 🗂️ Load Ground Truth | Read ground truth JSONs containing annotated bounding boxes |\n",
    "| 🧪 Build COCO GT | Convert ground truth to COCO format (`coco_gt.json`) |\n",
    "| 📈 Build COCO Results | Convert model predictions to COCO format (`coco_results.json`) |\n",
    "| 🔎 Evaluate | Run `pycocotools` evaluation and compute AP/AR metrics |\n",
    "| 📊 Summarize | Print detailed results and store them in a results dictionary |\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ 1. Configuration\n",
    "\n",
    "```python\n",
    "GT_DIR  = \"YourAnnotationsFolder\"           # Folder containing *_output.json files (ground truth)\n",
    "EXP_DIR = OUT_ROOT / \"frame_json\"           # Folder containing pipeline output JSONs (predictions)\n",
    "OUTDIR  = \"../coco_eval\"                    # Directory to save COCO-format JSONs and results\n",
    "CATEGORY_NAME = \"face\"                      # Object category name\n",
    "CATEGORY_ID = 1                             # Category ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#   (also accepts \"bbox_xyxy\" instead of \"pred_bbox\"; hulls ignored)\n",
    "all_results = {}\n",
    "\n",
    "import os, json, math, numpy as np\n",
    "from typing import Any, Dict, List, Tuple, Optional\n",
    "\n",
    "# ----------------- CONFIG: CHANGE THESE -----------------\n",
    "GT_DIR  = \"YourAnnotationsFolder\"    # folder containing *_output.json files\n",
    "EXP_DIR = OUT_ROOT / \"frame_json\"   # folder containing *.json files\n",
    "OUTDIR  = \"../coco_eval\"       # where coco_gt.json / coco_results.json will be written\n",
    "CATEGORY_NAME = \"face\"\n",
    "CATEGORY_ID = 1\n",
    "# --------------------------------------------------------\n",
    "\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "COCO_GT_PATH = os.path.join(OUTDIR, \"coco_gt.json\")\n",
    "COCO_DT_PATH = os.path.join(OUTDIR, \"coco_results.json\")\n",
    "\n",
    "# --------------- Helpers ---------------\n",
    "def list_jsons(folder: str) -> List[str]:\n",
    "    return sorted([f for f in os.listdir(folder) if f.lower().endswith(\".json\")])\n",
    "\n",
    "def read_json(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def write_json(obj, path: str):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f)\n",
    "\n",
    "def xyxy_to_xywh(b: List[float]) -> List[float]:\n",
    "    x1, y1, x2, y2 = [float(v) for v in b]\n",
    "    return [x1, y1, max(0.0, x2 - x1), max(0.0, y2 - y1)]\n",
    "\n",
    "def extract_all_gt_xyxy(js: Any) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    GT: list of dicts or single dict with 'bbox_xyxy'.\n",
    "    Returns all xyxy boxes. Empty list if none.\n",
    "    \"\"\"\n",
    "    out: List[List[float]] = []\n",
    "    if isinstance(js, dict):\n",
    "        bb = js.get(\"bbox_xyxy\")\n",
    "        if isinstance(bb, list) and len(bb) == 4:\n",
    "            out.append([float(x) for x in bb])\n",
    "    elif isinstance(js, list):\n",
    "        for item in js:\n",
    "            if isinstance(item, dict):\n",
    "                bb = item.get(\"bbox_xyxy\")\n",
    "                if isinstance(bb, list) and len(bb) == 4:\n",
    "                    out.append([float(x) for x in bb])\n",
    "    return out\n",
    "\n",
    "def parse_pred_rect_xyxy(det: Dict[str, Any]) -> Optional[List[float]]:\n",
    "    \"\"\"\n",
    "    EXP: prefer 'pred_bbox', fallback to 'bbox_xyxy', both xyxy.\n",
    "    \"\"\"\n",
    "    if not isinstance(det, dict):\n",
    "        return None\n",
    "    pb = det.get(\"pred_bbox\")\n",
    "    if isinstance(pb, list) and len(pb) == 4:\n",
    "        return [float(v) for v in pb]\n",
    "    bxyxy = det.get(\"bbox_xyxy\")\n",
    "    if isinstance(bxyxy, list) and len(bxyxy) == 4:\n",
    "        return [float(v) for v in bxyxy]\n",
    "    return None\n",
    "\n",
    "def extract_all_pred_xyxy_and_scores(js: Any) -> List[Tuple[List[float], float]]:\n",
    "    \"\"\"\n",
    "    EXP: list (or dict) of detections.\n",
    "    Score priority: det_score > score > land_score > 1.0\n",
    "    NaN/inf scores -> 0.0\n",
    "    \"\"\"\n",
    "    out: List[Tuple[List[float], float]] = []\n",
    "    dets = js if isinstance(js, list) else ([js] if isinstance(js, dict) else [])\n",
    "    for d in dets:\n",
    "        if not isinstance(d, dict):\n",
    "            continue\n",
    "        box = parse_pred_rect_xyxy(d)\n",
    "        if box is None:\n",
    "            continue\n",
    "        score = None\n",
    "        for k in (\"det_score\", \"score\", \"land_score\"):\n",
    "            v = d.get(k, None)\n",
    "            if isinstance(v, (int, float)):\n",
    "                score = float(v)\n",
    "                break\n",
    "        if score is None:\n",
    "            score = 1.0\n",
    "        # sanitize to finite\n",
    "        try:\n",
    "            if not math.isfinite(score):\n",
    "                score = 0.0\n",
    "        except Exception:\n",
    "            score = 0.0\n",
    "        out.append((box, score))\n",
    "    return out\n",
    "\n",
    "# --------------- Build COCO GT from GT_DIR ---------------\n",
    "def build_coco_gt_from_gt_folder(gt_dir: str, category_id: int = 1, category_name: str = \"face\") -> Dict:\n",
    "    \"\"\"\n",
    "    Creates COCO GT with:\n",
    "      images.file_name == exp filename (i.e., GT filename with trailing '_output' removed before '.json')\n",
    "      annotations with bbox in xywh\n",
    "      includes required 'info' and 'licenses' to satisfy pycocotools expectations\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    annotations = []\n",
    "    categories = [{\"id\": category_id, \"name\": category_name}]\n",
    "    ann_id = 1\n",
    "    img_id = 1\n",
    "\n",
    "    gt_files = list_jsons(gt_dir)\n",
    "    for fname in gt_files:\n",
    "        # Map GT \"<name>_output.json\" -> EXP \"<name>.json\"\n",
    "        if fname.endswith(\"_output.json\"):\n",
    "            exp_name = fname[:-len(\"_output.json\")] + \".json\"\n",
    "        else:\n",
    "            exp_name = fname  # fallback: identical name\n",
    "\n",
    "        img_entry = {\"id\": img_id, \"file_name\": exp_name}\n",
    "        images.append(img_entry)\n",
    "\n",
    "        js = read_json(os.path.join(gt_dir, fname))\n",
    "        gt_boxes = extract_all_gt_xyxy(js)\n",
    "        for b in gt_boxes:\n",
    "            x, y, w, h = xyxy_to_xywh(b)\n",
    "            annotations.append({\n",
    "                \"id\": ann_id,\n",
    "                \"image_id\": img_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": [x, y, w, h],\n",
    "                \"iscrowd\": 0,\n",
    "                \"area\": float(w * h),\n",
    "            })\n",
    "            ann_id += 1\n",
    "\n",
    "        img_id += 1\n",
    "\n",
    "    coco_gt = {\n",
    "        \"info\": {\n",
    "            \"description\": \"Auto-generated GT\",\n",
    "            \"version\": \"1.0\",\n",
    "            \"year\": 2025,\n",
    "            \"contributor\": \"\",\n",
    "            \"date_created\": \"\"\n",
    "        },\n",
    "        \"licenses\": [],\n",
    "        \"images\": images,\n",
    "        \"annotations\": annotations,\n",
    "        \"categories\": categories\n",
    "    }\n",
    "    return coco_gt\n",
    "\n",
    "# --------------- Build COCO Results from EXP_DIR using GT keying ---------------\n",
    "def build_coco_results_from_exp_folder(coco_gt: Dict, exp_dir: str, category_id: int = 1) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Uses coco_gt['images'] file_name to map to EXP files.\n",
    "    Returns list of detection dicts (xywh + score).\n",
    "    \"\"\"\n",
    "    # Map exp file_name -> image_id\n",
    "    name2id = {img[\"file_name\"]: img[\"id\"] for img in coco_gt.get(\"images\", [])}\n",
    "    results: List[Dict] = []\n",
    "\n",
    "    exp_files = set(list_jsons(exp_dir))\n",
    "    for exp_name, image_id in name2id.items():\n",
    "        if exp_name not in exp_files:\n",
    "            # missing prediction file: skip quietly\n",
    "            continue\n",
    "        js = read_json(os.path.join(exp_dir, exp_name))\n",
    "        preds = extract_all_pred_xyxy_and_scores(js)\n",
    "        for xyxy, score in preds:\n",
    "            x, y, w, h = xyxy_to_xywh(xyxy)\n",
    "            results.append({\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": [x, y, w, h],\n",
    "                \"score\": float(score),\n",
    "            })\n",
    "    return results\n",
    "\n",
    "# --------------- Save & Evaluate (pycocotools) ---------------\n",
    "def run_coco_eval(gt_path: str, dt_path: str):\n",
    "    from pycocotools.coco import COCO\n",
    "    from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "    cocoGt = COCO(gt_path)\n",
    "    cocoDt = cocoGt.loadRes(dt_path) if os.path.getsize(dt_path) > 2 else cocoGt.loadRes([])\n",
    "    img_ids = sorted(cocoGt.getImgIds())\n",
    "\n",
    "    # AP/AR on standard COCO range 0.50:0.95 (for reference)\n",
    "    ev_std = COCOeval(cocoGt, cocoDt, iouType='bbox')\n",
    "    ev_std.params.imgIds = img_ids\n",
    "    ev_std.evaluate(); ev_std.accumulate(); ev_std.summarize()\n",
    "    ap_std = ev_std.stats[0]  # AP@[.50:.95]\n",
    "\n",
    "    # AP@0.50\n",
    "    ev_50 = COCOeval(cocoGt, cocoDt, iouType='bbox')\n",
    "    ev_50.params.imgIds = img_ids\n",
    "    ev_50.params.iouThrs = np.array([0.50])\n",
    "    ev_50.evaluate(); ev_50.accumulate(); ev_50.summarize()\n",
    "    ap50 = ev_50.stats[0]  # AP at single IoU\n",
    "\n",
    "    # AP@0.75\n",
    "    ev_75 = COCOeval(cocoGt, cocoDt, iouType='bbox')\n",
    "    ev_75.params.imgIds = img_ids\n",
    "    ev_75.params.iouThrs = np.array([0.75])\n",
    "    ev_75.evaluate(); ev_75.accumulate(); ev_75.summarize()\n",
    "    ap75 = ev_75.stats[0]\n",
    "\n",
    "    # AP@[0.50:0.90] step 0.05\n",
    "    ev_5090 = COCOeval(cocoGt, cocoDt, iouType='bbox')\n",
    "    ev_5090.params.imgIds = img_ids\n",
    "    ev_5090.params.iouThrs = np.arange(0.50, 0.95, 0.05)  # 0.50,0.55,...,0.90\n",
    "    ev_5090.evaluate(); ev_5090.accumulate(); ev_5090.summarize()\n",
    "    ap_5090 = ev_5090.stats[0]\n",
    "\n",
    "    # AR@0.50\n",
    "    ev_ar50 = COCOeval(cocoGt, cocoDt, iouType='bbox')\n",
    "    ev_ar50.params.imgIds = img_ids\n",
    "    ev_ar50.params.iouThrs = np.array([0.50])\n",
    "    ev_ar50.evaluate(); ev_ar50.accumulate(); ev_ar50.summarize()\n",
    "    ar50 = ev_ar50.stats[8]  # AR (area=all, maxDets=100)\n",
    "\n",
    "    # AR@0.75\n",
    "    ev_ar75 = COCOeval(cocoGt, cocoDt, iouType='bbox')\n",
    "    ev_ar75.params.imgIds = img_ids\n",
    "    ev_ar75.params.iouThrs = np.array([0.75])\n",
    "    ev_ar75.evaluate(); ev_ar75.accumulate(); ev_ar75.summarize()\n",
    "    ar75 = ev_ar75.stats[8]\n",
    "\n",
    "    # AR@[0.50:0.90]\n",
    "    ev_ar5090 = COCOeval(cocoGt, cocoDt, iouType='bbox')\n",
    "    ev_ar5090.params.imgIds = img_ids\n",
    "    ev_ar5090.params.iouThrs = np.arange(0.50, 0.95, 0.05)\n",
    "    ev_ar5090.evaluate(); ev_ar5090.accumulate(); ev_ar5090.summarize()\n",
    "    ar_5090 = ev_ar5090.stats[8]\n",
    "\n",
    "    print(\"\\n======= Custom Summary =======\")\n",
    "    print(f\"AP@[0.50:0.95] : {ap_std:.4f}\")\n",
    "    print(f\"AP@0.50        : {ap50:.4f}\")\n",
    "    print(f\"AP@0.75        : {ap75:.4f}\")\n",
    "    print(f\"AP@[0.50:0.90] : {ap_5090:.4f}\")\n",
    "    print(f\"AR@0.50        : {ar50:.4f}\")\n",
    "    print(f\"AR@0.75        : {ar75:.4f}\")\n",
    "    print(f\"AR@[0.50:0.90] : {ar_5090:.4f}\")\n",
    "    print(\"================================\")\n",
    "\n",
    "    metrics = {\n",
    "        \"AP@[0.50:0.95]\": ap_std,\n",
    "        \"AP@0.50\": ap50,\n",
    "        \"AP@0.75\": ap75,\n",
    "        \"AP@[0.50:0.90]\": ap_5090,\n",
    "        \"AR@0.50\": ar50,\n",
    "        \"AR@0.75\": ar75,\n",
    "        \"AR@[0.50:0.90]\": ar_5090,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# --------------- Pipeline ---------------\n",
    "# 1) Build COCO GT from GT_DIR (mapping names by dropping trailing '_output')\n",
    "coco_gt = build_coco_gt_from_gt_folder(GT_DIR, category_id=CATEGORY_ID, category_name=CATEGORY_NAME)\n",
    "write_json(coco_gt, COCO_GT_PATH)\n",
    "print(f\"Wrote COCO GT to: {COCO_GT_PATH}  | images={len(coco_gt['images'])}, anns={len(coco_gt['annotations'])}\")\n",
    "\n",
    "# 2) Build COCO results from EXP_DIR keyed by coco_gt['images'].file_name\n",
    "coco_dt = build_coco_results_from_exp_folder(coco_gt, EXP_DIR, category_id=CATEGORY_ID)\n",
    "write_json(coco_dt, COCO_DT_PATH)\n",
    "try:\n",
    "    print(f\"Wrote COCO Results to: {COCO_DT_PATH}  | detections={len(coco_dt)} | images={len(coco_dt['images'])}\")\n",
    "except:\n",
    "    pass\n",
    "# 3) Install pycocotools (if not already) and evaluate\n",
    "try:\n",
    "    from pycocotools.coco import COCO  # noqa: F401\n",
    "except Exception:\n",
    "    !pip -q install pycocotools\n",
    "\n",
    "metrics = run_coco_eval(COCO_GT_PATH, COCO_DT_PATH)\n",
    "\n",
    "all_results[\"skip 0\"] = [\n",
    "    metrics[\"AP@0.50\"],\n",
    "    metrics[\"AP@0.75\"],\n",
    "    metrics[\"AP@[0.50:0.95]\"],\n",
    "    metrics[\"AP@[0.50:0.90]\"],\n",
    "    metrics[\"AR@0.50\"],\n",
    "    metrics[\"AR@0.75\"],\n",
    "    metrics[\"AR@[0.50:0.90]\"],\n",
    "]\n",
    "\n",
    "print(\"\\nUpdated results dict:\")\n",
    "print(all_results)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8144570,
     "sourceId": 13221078,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
