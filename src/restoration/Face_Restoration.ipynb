{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup\n"
      ],
      "metadata": {
        "id": "1bpI5iNZn0cn"
      },
      "id": "1bpI5iNZn0cn"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio\n",
        "!pip install torch==2.0.0 torchvision==0.15.1 torchaudio==2.0.0\n",
        "!pip uninstall -y numpy\n",
        "!pip install \"numpy<2\"\n",
        "!pip install onnxruntime\n",
        "!pip uninstall -y mediapipe\n",
        "!pip install mediapipe==0.10.5\n",
        "!pip install face_recognition opencv-python dlib\n",
        "!pip install basicsr facexlib\n",
        "!pip install insightface"
      ],
      "metadata": {
        "id": "uMvypdD2mrr0"
      },
      "id": "uMvypdD2mrr0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Converts Video into Frames\n"
      ],
      "metadata": {
        "id": "gl33i0_B5uIb"
      },
      "id": "gl33i0_B5uIb"
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Create output folder\n",
        "os.makedirs(\"/content/frames\", exist_ok=True)\n",
        "\n",
        "# Open the video\n",
        "video_path = \"/content/video.mp4\"  # <-- Change to your video path\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "# Check rotation\n",
        "rotate_code = None\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "if width > height:\n",
        "    rotate_code = cv2.ROTATE_90_CLOCKWISE\n",
        "\n",
        "frame_idx = 0\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Fix rotation if needed\n",
        "    if rotate_code is not None:\n",
        "        frame = cv2.rotate(frame, rotate_code)\n",
        "\n",
        "    # Save the frame as a JPG image\n",
        "    frame_filename = f\"/content/frames/frame_{frame_idx:04d}.jpg\"\n",
        "    cv2.imwrite(frame_filename, frame)\n",
        "\n",
        "    frame_idx += 1\n",
        "\n",
        "cap.release();\n",
        "print(frame_idx);\n",
        "print(\"Done splitting video into frames.\")\n"
      ],
      "metadata": {
        "id": "03wrcr28ad0k"
      },
      "id": "03wrcr28ad0k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Landmarks (txt format, x,y)"
      ],
      "metadata": {
        "id": "2DkBaseNmRHl"
      },
      "id": "2DkBaseNmRHl"
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "frames_folder = \"/content/frames\"  # ← change to your frames folder\n",
        "landmarks_folder = \"/content/landmarks_output\"  # ← output folder for .txt files\n",
        "os.makedirs(landmarks_folder, exist_ok=True)\n",
        "\n",
        "# MediaPipe setup\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True)\n",
        "\n",
        "# Loop over each frame\n",
        "for filename in sorted(os.listdir(frames_folder)):\n",
        "    if not filename.lower().endswith((\".jpg\", \".png\")):\n",
        "        continue\n",
        "\n",
        "    img_path = os.path.join(frames_folder, filename)\n",
        "    image = cv2.imread(img_path)\n",
        "    if image is None:\n",
        "        print(f\"Failed to read image: {filename}\")\n",
        "        continue\n",
        "\n",
        "    # Convert BGR to RGB\n",
        "    rgb_img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = face_mesh.process(rgb_img)\n",
        "\n",
        "    if results.multi_face_landmarks:\n",
        "        face_landmarks = results.multi_face_landmarks[0]\n",
        "        h, w = image.shape[:2]\n",
        "\n",
        "        # Create a .txt file for each frame\n",
        "        txt_filename = os.path.splitext(filename)[0] + \".txt\"\n",
        "        txt_path = os.path.join(landmarks_folder, txt_filename)\n",
        "\n",
        "        with open(txt_path, \"w\") as f:\n",
        "            for landmark in face_landmarks.landmark:\n",
        "                x = int(landmark.x * w)\n",
        "                y = int(landmark.y * h)\n",
        "                f.write(f\"{x},{y}\\n\")\n",
        "\n",
        "        print(f\"Saved landmarks for: {filename}\")\n",
        "    else:\n",
        "        print(f\"No face detected in: {filename}\")\n",
        "\n",
        "print(\"✅ Landmark extraction complete.\")\n"
      ],
      "metadata": {
        "id": "0tSjb5VEmPmI"
      },
      "id": "0tSjb5VEmPmI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OR Upload landmarks"
      ],
      "metadata": {
        "id": "FmrgK8yymlbR"
      },
      "id": "FmrgK8yymlbR"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Upload the zip file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract the uploaded zip file\n",
        "zip_file = list(uploaded.keys())[0]\n",
        "output_dir = '/content/'  # Folder where you want to extract\n",
        "\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_dir)\n",
        "\n",
        "print(f\"Files extracted to {output_dir}\")\n"
      ],
      "metadata": {
        "id": "Ya0da4YtjUYs"
      },
      "id": "Ya0da4YtjUYs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check if landmarks are fine"
      ],
      "metadata": {
        "id": "kdW4Py6vzVFu"
      },
      "id": "kdW4Py6vzVFu"
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "folder_path = '/content/landmarks'  # Replace with your folder path\n",
        "output_path = '/content/output_images'  # Where to save visualizations\n",
        "canvas_size = (512, 512)  # Width, Height\n",
        "radius = 0  # Dot radius\n",
        "draw_connections = False  # Set True to draw mesh lines\n",
        "\n",
        "# (Optional) Define connections based on MediaPipe's FaceMesh (partial list for demo)\n",
        "FACEMESH_CONNECTIONS = [\n",
        "    (10, 338), (338, 297), (297, 332), (332, 284), (284, 251),\n",
        "    # Add more connections if needed...\n",
        "]\n",
        "\n",
        "# === FUNCTIONS ===\n",
        "def load_landmarks(filepath):\n",
        "    with open(filepath, 'r') as f:\n",
        "        return [tuple(map(float, line.strip().split(','))) for line in f.readlines()]\n",
        "\n",
        "def draw_landmarks(landmarks, size=(512, 512), radius=1, connections=False):\n",
        "    canvas = np.zeros((size[1], size[0], 3), dtype=np.uint8)\n",
        "    for x, y in landmarks:\n",
        "        cx, cy = int(x), int(y)\n",
        "        cv2.circle(canvas, (cx, cy), radius, (0, 255, 0), -1)\n",
        "\n",
        "    if connections:\n",
        "        for i, j in FACEMESH_CONNECTIONS:\n",
        "            pt1 = tuple(map(int, landmarks[i]))\n",
        "            pt2 = tuple(map(int, landmarks[j]))\n",
        "            cv2.line(canvas, pt1, pt2, (255, 0, 0), 1)\n",
        "\n",
        "    return canvas\n",
        "\n",
        "# === MAIN ===\n",
        "os.makedirs(output_path, exist_ok=True)\n",
        "landmark_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.txt')])\n",
        "\n",
        "for idx, filename in enumerate(landmark_files):\n",
        "    path = os.path.join(folder_path, filename)\n",
        "    landmarks = load_landmarks(path)\n",
        "    image = draw_landmarks(landmarks, size=canvas_size, radius=radius, connections=draw_connections)\n",
        "\n",
        "    output_filename = os.path.join(output_path, f\"frame_{idx:04d}.png\")\n",
        "    cv2.imwrite(output_filename, image)\n",
        "    print(f\"Saved: {output_filename}\")\n"
      ],
      "metadata": {
        "id": "feSpq-xuzQ1h"
      },
      "id": "feSpq-xuzQ1h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch Reconstruction using GFPGAN"
      ],
      "metadata": {
        "id": "GIfPrVSd5iaO"
      },
      "id": "GIfPrVSd5iaO"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/TencentARC/GFPGAN.git\n",
        "%cd GFPGAN\n"
      ],
      "metadata": {
        "id": "shN7MKBQT-u8"
      },
      "id": "shN7MKBQT-u8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a886d231",
      "metadata": {
        "id": "a886d231"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from gfpgan import GFPGANer\n",
        "from basicsr.utils.download_util import load_file_from_url\n",
        "from insightface.app import FaceAnalysis\n",
        "from insightface.utils import face_align\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# 1️⃣ Load GFPGAN Model\n",
        "model = load_file_from_url(\n",
        "    'https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth',\n",
        "    model_dir='gfpgan/weights', progress=True)\n",
        "\n",
        "restorer = GFPGANer(model_path=model,\n",
        "                    upscale=1, arch='clean',\n",
        "                    channel_multiplier=2, bg_upsampler=None)\n",
        "\n",
        "# 2️⃣ Set Paths for Input Folders\n",
        "image_folder = '/content/frames'  # Folder with image frames\n",
        "landmark_folder = '/content/landmarks'  # Folder with txt landmark files\n",
        "\n",
        "# Create output folders\n",
        "os.makedirs('/content/restored_images', exist_ok=True)\n",
        "\n",
        "# 3️⃣ Align & Crop Function (for one image)\n",
        "def align_and_crop(image_path, landmarks_path):\n",
        "    img = cv2.imread(image_path)\n",
        "    lm = pd.read_csv(landmarks_path, header=None)  # read raw txt format\n",
        "    lm.columns = ['X', 'Y']  # manually assign column names\n",
        "    lm = lm[['X', 'Y']].values\n",
        "\n",
        "\n",
        "    # Align & crop based on landmarks\n",
        "    h, w = img.shape[:2]\n",
        "    pts = (lm * [w, h]).astype(int)\n",
        "    x0, y0 = np.clip(pts.min(0) - 20, 0, [w, h])\n",
        "    x1, y1 = np.clip(pts.max(0) + 20, 0, [w, h])\n",
        "    crop = img[y0:y1, x0:x1]\n",
        "\n",
        "    return crop, img, lm\n",
        "\n",
        "# Process All Images in the Folder\n",
        "for filename in os.listdir(image_folder):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "        base_name = os.path.splitext(filename)[0]\n",
        "        landmark_filename = f\"{base_name}_face0_landmarks.txt\"\n",
        "\n",
        "        landmark_path = os.path.join(landmark_folder, landmark_filename)\n",
        "\n",
        "        if not os.path.exists(landmark_path):\n",
        "            print(f\"Warning: No landmarks found for {filename}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Align and crop face\n",
        "        crop, full_img, lm = align_and_crop(image_path, landmark_path)\n",
        "\n",
        "        # GFPGAN Enhancement\n",
        "        out = restorer.enhance(full_img, has_aligned=False, only_center_face=True)\n",
        "        if len(out) == 3:\n",
        "            _, faces, restored_img = out\n",
        "        else:\n",
        "            faces, restored_img = out\n",
        "\n",
        "        # Save restored image\n",
        "        restored_filename = os.path.join('restored_images', f\"restored_{filename}\")\n",
        "        cv2.imwrite(restored_filename, restored_img)\n",
        "\n",
        "        # Reload landmarks for drawing on restored image\n",
        "        h, w = restored_img.shape[:2]\n",
        "        pts = (lm * [w, h]).astype(int)\n",
        "\n",
        "        # Draw landmarks on restored image\n",
        "        for (x, y) in pts:\n",
        "            cv2.circle(restored_img, (x, y), 1, (0, 255, 0), -1)  # Green dots\n",
        "\n",
        "        # Save and display the image with landmarks\n",
        "        restored_with_landmarks_filename = os.path.join('restored_images', f\"restored_with_landmarks_{filename}\")\n",
        "        cv2.imwrite(restored_with_landmarks_filename, restored_img)\n",
        "\n",
        "        # Display the image (optional for debugging)\n",
        "        display(Image(restored_with_landmarks_filename))\n",
        "\n",
        "print(\"Batch reconstruction complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Reconstructed Frames into Video"
      ],
      "metadata": {
        "id": "ViVKf8T354oH"
      },
      "id": "ViVKf8T354oH"
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "# Set the path to the folder containing the processed frames\n",
        "frames_folder = \"/content/GFPGAN/restored_images\"  # Path to your folder of frames\n",
        "output_video_path = \"/content/reconstructed_video.mp4\"  # Name of the output video file\n",
        "\n",
        "# Get all the image files in the frames folder and sort them numerically\n",
        "frame_files = sorted([f for f in os.listdir(frames_folder) if f.endswith(\".jpg\") and \"landmarks\" not in f], key=lambda x: int(x.split(\"_\")[2].split(\".\")[0]))\n",
        "\n",
        "# Read the first frame to get the video frame size (width, height)\n",
        "first_frame = cv2.imread(os.path.join(frames_folder, frame_files[0]))\n",
        "height, width, _ = first_frame.shape\n",
        "\n",
        "# Define the video writer object\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # You can change 'mp4v' to another codec if needed\n",
        "fps = 30  # Set the frames per second of the output video\n",
        "\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "# Loop through all the frame files and write them to the video\n",
        "for frame_file in frame_files:\n",
        "    frame_path = os.path.join(frames_folder, frame_file)\n",
        "    frame = cv2.imread(frame_path)\n",
        "\n",
        "    # Write the frame to the video\n",
        "    out.write(frame)\n",
        "\n",
        "# Release the video writer and finalize the video\n",
        "out.release()\n",
        "\n",
        "print(f\"Video successfully saved as {output_video_path}\")\n"
      ],
      "metadata": {
        "id": "gFY6KSR6kEgl"
      },
      "id": "gFY6KSR6kEgl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Face Swap (Optional - using Delauney's Triangulation and warping)\n",
        "\n"
      ],
      "metadata": {
        "id": "Mdt_Q7Sy6AP2"
      },
      "id": "Mdt_Q7Sy6AP2"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install face_recognition opencv-python dlib"
      ],
      "metadata": {
        "id": "lN8NJnIZCuMk"
      },
      "id": "lN8NJnIZCuMk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import face_recognition\n",
        "import os\n",
        "\n",
        "def extract_face_landmarks(image):\n",
        "    # Returns landmarks dict for first face detected\n",
        "    face_landmarks_list = face_recognition.face_landmarks(image)\n",
        "    if len(face_landmarks_list) == 0:\n",
        "        return None\n",
        "    return face_landmarks_list[0]\n",
        "\n",
        "def create_face_mask(image, landmarks):\n",
        "    mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
        "    points = []\n",
        "    for feature in landmarks.values():\n",
        "        points += feature\n",
        "    points = np.array(points, dtype=np.int32)\n",
        "    cv2.fillConvexPoly(mask, cv2.convexHull(points), 255)\n",
        "    return mask\n",
        "\n",
        "def warp_face(src_face, src_points, dst_points, dst_shape):\n",
        "    src_points = np.array(src_points, dtype=np.float32)\n",
        "    dst_points = np.array(dst_points, dtype=np.float32)\n",
        "    # Compute affine transform\n",
        "    M, _ = cv2.estimateAffinePartial2D(src_points, dst_points)\n",
        "    warped = cv2.warpAffine(src_face, M, (dst_shape[1], dst_shape[0]), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT_101)\n",
        "    return warped\n",
        "\n",
        "def swap_face(src_img, dst_img):\n",
        "    src_landmarks = extract_face_landmarks(src_img)\n",
        "    dst_landmarks = extract_face_landmarks(dst_img)\n",
        "\n",
        "    if src_landmarks is None or dst_landmarks is None:\n",
        "        print(\"Face not detected in one of the images. Skipping frame.\")\n",
        "        return dst_img\n",
        "\n",
        "    # Create face masks\n",
        "    src_mask = create_face_mask(src_img, src_landmarks)\n",
        "    dst_mask = create_face_mask(dst_img, dst_landmarks)\n",
        "\n",
        "    # Get points for warp (using chin, eyes, nose, mouth approx)\n",
        "    src_points = []\n",
        "    dst_points = []\n",
        "    for feature in ['chin', 'left_eye', 'right_eye', 'nose_bridge', 'top_lip', 'bottom_lip']:\n",
        "        src_points += src_landmarks[feature]\n",
        "        dst_points += dst_landmarks[feature]\n",
        "\n",
        "    # Warp source face to destination shape\n",
        "    warped_src_face = warp_face(src_img, src_points, dst_points, dst_img.shape)\n",
        "\n",
        "    # Combine warped face into destination image using masks\n",
        "    # Mask of warped face on destination\n",
        "    warped_mask = warp_face(src_mask, src_points, dst_points, dst_img.shape)\n",
        "    warped_mask = cv2.merge([warped_mask, warped_mask, warped_mask])\n",
        "\n",
        "    # Blend faces\n",
        "    warped_mask = warped_mask / 255.0\n",
        "    output = dst_img * (1 - warped_mask) + warped_src_face * warped_mask\n",
        "    output = output.astype(np.uint8)\n",
        "\n",
        "    return output\n",
        "\n",
        "src_img_path = \"/content/OG.jpg\" # <-- Change to your source image path\n",
        "dst_folder = \"/content/frames\" # <-- Change to your dst image folder\n",
        "output_folder = \"/content/swapped_frames\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Load source image once\n",
        "src_img = cv2.imread(src_img_path)\n",
        "if src_img is None:\n",
        "    raise ValueError(f\"Could not load source image: {src_img_path}\")\n",
        "\n",
        "# Extract landmarks for source image once (optional optimization)\n",
        "src_landmarks = extract_face_landmarks(src_img)\n",
        "if src_landmarks is None:\n",
        "    raise ValueError(\"No face detected in source image!\")\n",
        "\n",
        "# Get destination frames list\n",
        "dst_files = sorted([f for f in os.listdir(dst_folder) if f.endswith(('.png', '.jpg'))])\n",
        "\n",
        "for f in dst_files:\n",
        "    dst_img_path = os.path.join(dst_folder, f)\n",
        "    dst_img = cv2.imread(dst_img_path)\n",
        "    if dst_img is None:\n",
        "        print(f\"Skipping {f}: failed to load destination image.\")\n",
        "        continue\n",
        "\n",
        "    # Modify swap_face to optionally accept precomputed src_landmarks to speed up\n",
        "    swapped = swap_face(src_img, dst_img)\n",
        "\n",
        "    output_path = os.path.join(output_folder, f)\n",
        "    cv2.imwrite(output_path, swapped)\n",
        "    print(f\"Swapped and saved frame: {f}\")\n",
        "\n",
        "print(\"Batch face swap with single source image complete!\")"
      ],
      "metadata": {
        "id": "dL-dgxZxnLWw"
      },
      "id": "dL-dgxZxnLWw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}